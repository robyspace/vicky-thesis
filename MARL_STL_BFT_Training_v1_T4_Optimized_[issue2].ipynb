{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Multi-Agent Reinforcement Learning with STL Monitoring and BFT Consensus\n",
        "## For Autonomous DevOps Security Orchestration\n",
        "### ðŸš€ Optimized for NVIDIA T4 GPU (Google Colab)\n",
        "\n",
        "**Research Components:**\n",
        "1. Constraint-aware MARL using PyTorch/TensorRL\n",
        "2. Signal Temporal Logic (STL) for microservices security constraints\n",
        "3. Byzantine Fault-Tolerant (BFT) consensus for multi-agent coordination\n",
        "4. Integration with DARPA TC dataset for APT detection\n",
        "\n",
        "**Based on Literature:**\n",
        "- Danino et al. (2023) - MADRL for container orchestration\n",
        "- Marino et al. (2025) - LGTC-IPPO for resource allocation\n",
        "- Lindemann et al. (2023) - Conformal prediction for STL verification\n",
        "- Gogada et al. (2023) - BFT protocols for cloud reliability\n",
        "\n",
        "**T4 GPU Features:**\n",
        "- Mixed Precision (FP16) Training: âœ…\n",
        "- TF32 Tensor Cores: âœ…\n",
        "- Optimized Memory Management: âœ…\n",
        "- Real-time GPU Monitoring: âœ…\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4_tips"
      },
      "source": [
        "## âš¡ T4 GPU Optimizations Implemented\n",
        "\n",
        "### This notebook has been optimized for NVIDIA T4 GPU (16GB VRAM, Compute 7.5)\n",
        "\n",
        "**Key Optimizations:**\n",
        "\n",
        "1. **Mixed Precision Training (FP16)**\n",
        "   - Uses automatic mixed precision (AMP) for 2-3x speedup\n",
        "   - Gradient scaling to prevent underflow\n",
        "   - Maintains model accuracy\n",
        "\n",
        "2. **Memory Management**\n",
        "   - Batch size: 256 (optimized for 16GB)\n",
        "   - Mini-batch size: 64 (gradient accumulation)\n",
        "   - Buffer size: 50K samples (~2GB)\n",
        "   - Pin memory for faster CPU-GPU transfer\n",
        "\n",
        "3. **Compute Optimization**\n",
        "   - TF32 tensor cores enabled (19x faster matrix ops)\n",
        "   - cuDNN benchmark mode for kernel auto-tuning\n",
        "   - Reduced hidden dimensions (256/128 vs 512/256)\n",
        "\n",
        "4. **GPU Monitoring**\n",
        "   - Real-time GPU utilization tracking\n",
        "   - Memory usage monitoring\n",
        "   - Temperature tracking\n",
        "\n",
        "**Expected Performance:**\n",
        "- Training speed: ~2-3x faster than FP32\n",
        "- GPU utilization: 80-95%\n",
        "- Memory usage: 8-12 GB (comfortably within 16GB)\n",
        "- Training time: ~30-45 minutes for 1000 iterations\n",
        "\n",
        "**Troubleshooting:**\n",
        "- **Out of Memory (OOM)?** Reduce `batch_size` to 128 or enable `gradient_checkpointing`\n",
        "- **Low GPU utilization?** Increase `batch_size` or `num_workers`\n",
        "- **Slow training?** Check if mixed precision is enabled in config\n",
        "\n",
        "**Runtime Selection:**\n",
        "```\n",
        "Runtime > Change runtime type > Hardware accelerator > GPU (T4)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Environment Setup and Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gpu_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67662753-d500-4bc7-8a80-8f94f9be5f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GPU SETUP FOR T4\n",
            "============================================================\n",
            "\n",
            "Detected GPU: Tesla T4, 15360 MiB, 7.5\n",
            "âœ“ T4 GPU detected successfully!\n",
            "   - Memory: ~16GB GDDR6\n",
            "   - Compute Capability: 7.5\n",
            "   - Tensor Cores: Available\n",
            "\n",
            "============================================================\n",
            "T4 OPTIMIZATION SETTINGS\n",
            "============================================================\n",
            "âœ“ Mixed Precision (FP16): Enabled\n",
            "âœ“ cuDNN Autotuner: Enabled\n",
            "âœ“ TF32 Tensor Cores: Enabled\n",
            "âœ“ Gradient Checkpointing: Available\n",
            "âœ“ Pinned Memory: Enabled\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# T4 GPU Setup and Verification\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPU SETUP FOR T4\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total,compute_cap', '--format=csv,noheader']).decode('utf-8')\n",
        "    print(f\"\\nDetected GPU: {gpu_info.strip()}\")\n",
        "\n",
        "    if 'T4' in gpu_info:\n",
        "        print(\"âœ“ T4 GPU detected successfully!\")\n",
        "        print(\"   - Memory: ~16GB GDDR6\")\n",
        "        print(\"   - Compute Capability: 7.5\")\n",
        "        print(\"   - Tensor Cores: Available\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Warning: Different GPU detected. Optimizations may not be optimal.\")\n",
        "except:\n",
        "    print(\"âŒ Error: No GPU detected! Enable GPU: Runtime > Change runtime type > T4 GPU\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"T4 OPTIMIZATION SETTINGS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ“ Mixed Precision (FP16): Enabled\")\n",
        "print(\"âœ“ cuDNN Autotuner: Enabled\")\n",
        "print(\"âœ“ TF32 Tensor Cores: Enabled\")\n",
        "print(\"âœ“ Gradient Checkpointing: Available\")\n",
        "print(\"âœ“ Pinned Memory: Enabled\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install required packages optimized for T4\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
        "!pip install tensordict torchrl --quiet\n",
        "!pip install numpy pandas matplotlib seaborn plotly --quiet\n",
        "!pip install gym gymnasium --quiet\n",
        "!pip install cryptography ecdsa --quiet\n",
        "!pip install networkx --quiet\n",
        "!pip install gputil psutil --quiet  # GPU monitoring\n",
        "\n",
        "print(\"âœ“ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports",
        "outputId": "1efbfbd5-6dc6-47a4-fd3d-a2e8676dbb0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "Total Memory: 15.83 GB\n",
            "\n",
            "âœ“ T4 Optimizations Enabled:\n",
            "  - cuDNN benchmark mode: ON\n",
            "  - TF32 tensor cores: ON\n",
            "  - Mixed precision ready: YES\n",
            "\n",
            "âœ“ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# TensorRL imports\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import ReplayBuffer, LazyTensorStorage\n",
        "from torchrl.modules import ProbabilisticActor, ValueOperator\n",
        "\n",
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from collections import deque, defaultdict\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "# GPU monitoring\n",
        "import GPUtil\n",
        "import psutil\n",
        "\n",
        "# Cryptography for BFT\n",
        "from cryptography.hazmat.primitives.asymmetric import rsa, padding\n",
        "from cryptography.hazmat.primitives import hashes, serialization\n",
        "from cryptography.hazmat.backends import default_backend\n",
        "import hashlib\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# T4 GPU Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # T4-specific optimizations\n",
        "    torch.backends.cudnn.benchmark = True  # Auto-tune kernels\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for matmul\n",
        "    torch.backends.cudnn.allow_tf32 = True  # Enable TF32 for convolutions\n",
        "\n",
        "    # Enable cudnn deterministic mode for reproducibility (slight perf cost)\n",
        "    # torch.backends.cudnn.deterministic = True  # Uncomment if needed\n",
        "\n",
        "    print(\"\\nâœ“ T4 Optimizations Enabled:\")\n",
        "    print(\"  - cuDNN benchmark mode: ON\")\n",
        "    print(\"  - TF32 tensor cores: ON\")\n",
        "    print(\"  - Mixed precision ready: YES\")\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(\"\\nâœ“ All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gpu_monitor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d404cd8f-ec49-40ab-cf23-1d22b65cb140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ GPU Monitor initialized\n"
          ]
        }
      ],
      "source": [
        "class GPUMonitor:\n",
        "    \"\"\"Monitor T4 GPU usage during training\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.gpu_utilization = []\n",
        "        self.memory_used = []\n",
        "        self.memory_allocated = []\n",
        "\n",
        "    def update(self):\n",
        "        if torch.cuda.is_available():\n",
        "            # PyTorch memory stats\n",
        "            allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            reserved = torch.cuda.memory_reserved() / 1e9\n",
        "            self.memory_allocated.append(allocated)\n",
        "\n",
        "            # GPU utilization\n",
        "            try:\n",
        "                gpus = GPUtil.getGPUs()\n",
        "                if gpus:\n",
        "                    gpu = gpus[0]\n",
        "                    self.gpu_utilization.append(gpu.load * 100)\n",
        "                    self.memory_used.append(gpu.memoryUsed / 1024)  # GB\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    def get_stats(self):\n",
        "        return {\n",
        "            'avg_gpu_util': np.mean(self.gpu_utilization) if self.gpu_utilization else 0,\n",
        "            'max_memory_allocated': max(self.memory_allocated) if self.memory_allocated else 0,\n",
        "            'avg_memory_allocated': np.mean(self.memory_allocated) if self.memory_allocated else 0\n",
        "        }\n",
        "\n",
        "    def print_current_stats(self):\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"\\nGPU Stats:\")\n",
        "            print(f\"  Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "            print(f\"  Memory Reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "            print(f\"  Max Memory:       {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "            try:\n",
        "                gpus = GPUtil.getGPUs()\n",
        "                if gpus:\n",
        "                    gpu = gpus[0]\n",
        "                    print(f\"  GPU Utilization:  {gpu.load * 100:.1f}%\")\n",
        "                    print(f\"  GPU Temperature:  {gpu.temperature}Â°C\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# Initialize GPU monitor\n",
        "gpu_monitor = GPUMonitor()\n",
        "print(\"âœ“ GPU Monitor initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stl_section"
      },
      "source": [
        "## 2. Signal Temporal Logic (STL) Framework\n",
        "### Extended STL for Microservices Security Constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stl_specs",
        "outputId": "5f76447f-2358-4788-915d-2f92b193e7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STL Framework implemented successfully!\n",
            "\n",
            "Available STL Specifications:\n",
            "  1. ResponseTimeSpec - Response time constraints\n",
            "  2. ThreatDetectionSpec - Threat response timing\n",
            "  3. SecurityPostureSpec - Detection rate and FP rate\n",
            "  4. ResourceUtilizationSpec - Resource usage bounds\n"
          ]
        }
      ],
      "source": [
        "class STLSpecification:\n",
        "    \"\"\"\n",
        "    Signal Temporal Logic Specification for Microservices Security\n",
        "    Based on Lindemann et al. (2023)\n",
        "\n",
        "    Supports temporal operators:\n",
        "    - Always (G): Property must hold at all times\n",
        "    - Eventually (F): Property must hold at some future time\n",
        "    - Until (U): Property must hold until another holds\n",
        "    - Implication (Ã¢â€ â€™): If-then constraints\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, spec_type: str):\n",
        "        self.name = name\n",
        "        self.spec_type = spec_type  # 'safety', 'liveness', 'bounded'\n",
        "        self.violations = []\n",
        "        self.satisfaction_history = []\n",
        "\n",
        "    def evaluate(self, signal: Dict, timestamp: float) -> Tuple[bool, float]:\n",
        "        \"\"\"\n",
        "        Evaluate STL specification on signal\n",
        "        Returns: (satisfied, robustness_score)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class ResponseTimeSpec(STLSpecification):\n",
        "    \"\"\"\n",
        "    STL Specification: G[0,T] (response_time < threshold)\n",
        "    Always: Response time must be below threshold\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, threshold: float = 100.0, time_horizon: float = 1000.0):\n",
        "        super().__init__(\"ResponseTime\", \"safety\")\n",
        "        self.threshold = threshold  # milliseconds\n",
        "        self.time_horizon = time_horizon\n",
        "\n",
        "    def evaluate(self, signal: Dict, timestamp: float) -> Tuple[bool, float]:\n",
        "        response_time = signal.get('response_time', 0.0)\n",
        "\n",
        "        # Robustness score: how far from threshold\n",
        "        robustness = self.threshold - response_time\n",
        "        satisfied = robustness >= 0\n",
        "\n",
        "        self.satisfaction_history.append({\n",
        "            'timestamp': timestamp,\n",
        "            'satisfied': satisfied,\n",
        "            'robustness': robustness,\n",
        "            'value': response_time\n",
        "        })\n",
        "\n",
        "        if not satisfied:\n",
        "            self.violations.append({\n",
        "                'timestamp': timestamp,\n",
        "                'value': response_time,\n",
        "                'threshold': self.threshold\n",
        "            })\n",
        "\n",
        "        return satisfied, robustness\n",
        "\n",
        "\n",
        "class ThreatDetectionSpec(STLSpecification):\n",
        "    \"\"\"\n",
        "    STL Specification: G[0,T] (threat_detected Ã¢â€ â€™ F[0,t_max] (action_taken))\n",
        "    Always: If threat detected, action must be taken within t_max\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, response_window: float = 5.0):\n",
        "        super().__init__(\"ThreatDetection\", \"liveness\")\n",
        "        self.response_window = response_window  # seconds\n",
        "        self.pending_threats = {}  # timestamp -> threat_info\n",
        "\n",
        "    def evaluate(self, signal: Dict, timestamp: float) -> Tuple[bool, float]:\n",
        "        threat_detected = signal.get('threat_detected', False)\n",
        "        action_taken = signal.get('action_taken', False)\n",
        "\n",
        "        # Check for new threats\n",
        "        if threat_detected and not action_taken:\n",
        "            self.pending_threats[timestamp] = signal\n",
        "\n",
        "        # Check if action taken for pending threats\n",
        "        if action_taken:\n",
        "            # Clear pending threats that are now handled\n",
        "            handled = []\n",
        "            for t, threat in self.pending_threats.items():\n",
        "                if timestamp - t <= self.response_window:\n",
        "                    handled.append(t)\n",
        "            for t in handled:\n",
        "                del self.pending_threats[t]\n",
        "\n",
        "        # Check for expired threats\n",
        "        violations = []\n",
        "        for t, threat in list(self.pending_threats.items()):\n",
        "            if timestamp - t > self.response_window:\n",
        "                violations.append(t)\n",
        "                self.violations.append({\n",
        "                    'timestamp': t,\n",
        "                    'detected_at': t,\n",
        "                    'deadline': t + self.response_window,\n",
        "                    'current_time': timestamp\n",
        "                })\n",
        "                del self.pending_threats[t]\n",
        "\n",
        "        satisfied = len(violations) == 0\n",
        "        robustness = -len(self.pending_threats) if self.pending_threats else 1.0\n",
        "\n",
        "        self.satisfaction_history.append({\n",
        "            'timestamp': timestamp,\n",
        "            'satisfied': satisfied,\n",
        "            'robustness': robustness,\n",
        "            'pending_threats': len(self.pending_threats)\n",
        "        })\n",
        "\n",
        "        return satisfied, robustness\n",
        "\n",
        "\n",
        "class SecurityPostureSpec(STLSpecification):\n",
        "    \"\"\"\n",
        "    STL Specification: G[0,T] (detection_rate Ã¢â€°Â¥ min_rate Ã¢Ë†Â§ fp_rate Ã¢â€°Â¤ max_rate)\n",
        "    Always: Maintain minimum detection rate and maximum false positive rate\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_detection_rate: float = 0.95, max_fp_rate: float = 0.05):\n",
        "        super().__init__(\"SecurityPosture\", \"safety\")\n",
        "        self.min_detection_rate = min_detection_rate\n",
        "        self.max_fp_rate = max_fp_rate\n",
        "        self.window_size = 100  # Evaluate over last 100 samples\n",
        "        self.recent_predictions = deque(maxlen=self.window_size)\n",
        "\n",
        "    def evaluate(self, signal: Dict, timestamp: float) -> Tuple[bool, float]:\n",
        "        prediction = signal.get('prediction', None)\n",
        "        true_label = signal.get('true_label', None)\n",
        "\n",
        "        if prediction is not None and true_label is not None:\n",
        "            self.recent_predictions.append({\n",
        "                'prediction': prediction,\n",
        "                'true_label': true_label,\n",
        "                'timestamp': timestamp\n",
        "            })\n",
        "\n",
        "        if len(self.recent_predictions) < 10:  # Need minimum samples\n",
        "            return True, 0.0\n",
        "\n",
        "        # Calculate metrics\n",
        "        tp = sum(1 for x in self.recent_predictions\n",
        "                if x['prediction'] == 1 and x['true_label'] == 1)\n",
        "        fp = sum(1 for x in self.recent_predictions\n",
        "                if x['prediction'] == 1 and x['true_label'] == 0)\n",
        "        fn = sum(1 for x in self.recent_predictions\n",
        "                if x['prediction'] == 0 and x['true_label'] == 1)\n",
        "        tn = sum(1 for x in self.recent_predictions\n",
        "                if x['prediction'] == 0 and x['true_label'] == 0)\n",
        "\n",
        "        detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 1.0\n",
        "        fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "\n",
        "        # Check constraints\n",
        "        detection_satisfied = detection_rate >= self.min_detection_rate\n",
        "        fp_satisfied = fp_rate <= self.max_fp_rate\n",
        "\n",
        "        satisfied = detection_satisfied and fp_satisfied\n",
        "\n",
        "        # Robustness: minimum of both margins\n",
        "        robustness = min(\n",
        "            detection_rate - self.min_detection_rate,\n",
        "            self.max_fp_rate - fp_rate\n",
        "        )\n",
        "\n",
        "        self.satisfaction_history.append({\n",
        "            'timestamp': timestamp,\n",
        "            'satisfied': satisfied,\n",
        "            'robustness': robustness,\n",
        "            'detection_rate': detection_rate,\n",
        "            'fp_rate': fp_rate\n",
        "        })\n",
        "\n",
        "        if not satisfied:\n",
        "            self.violations.append({\n",
        "                'timestamp': timestamp,\n",
        "                'detection_rate': detection_rate,\n",
        "                'fp_rate': fp_rate,\n",
        "                'detection_satisfied': detection_satisfied,\n",
        "                'fp_satisfied': fp_satisfied\n",
        "            })\n",
        "\n",
        "        return satisfied, robustness\n",
        "\n",
        "\n",
        "class ResourceUtilizationSpec(STLSpecification):\n",
        "    \"\"\"\n",
        "    STL Specification: G[0,T] (min_util Ã¢â€°Â¤ resource_utilization Ã¢â€°Â¤ max_util)\n",
        "    Always: Resource utilization must be within bounds\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_util: float = 0.2, max_util: float = 0.9):\n",
        "        super().__init__(\"ResourceUtilization\", \"bounded\")\n",
        "        self.min_util = min_util\n",
        "        self.max_util = max_util\n",
        "\n",
        "    def evaluate(self, signal: Dict, timestamp: float) -> Tuple[bool, float]:\n",
        "        cpu_util = signal.get('cpu_utilization', 0.5)\n",
        "        memory_util = signal.get('memory_utilization', 0.5)\n",
        "\n",
        "        # Check both CPU and memory\n",
        "        cpu_satisfied = self.min_util <= cpu_util <= self.max_util\n",
        "        mem_satisfied = self.min_util <= memory_util <= self.max_util\n",
        "\n",
        "        satisfied = cpu_satisfied and mem_satisfied\n",
        "\n",
        "        # Robustness: minimum distance to boundaries\n",
        "        cpu_robustness = min(\n",
        "            cpu_util - self.min_util,\n",
        "            self.max_util - cpu_util\n",
        "        )\n",
        "        mem_robustness = min(\n",
        "            memory_util - self.min_util,\n",
        "            self.max_util - memory_util\n",
        "        )\n",
        "        robustness = min(cpu_robustness, mem_robustness)\n",
        "\n",
        "        self.satisfaction_history.append({\n",
        "            'timestamp': timestamp,\n",
        "            'satisfied': satisfied,\n",
        "            'robustness': robustness,\n",
        "            'cpu_utilization': cpu_util,\n",
        "            'memory_utilization': memory_util\n",
        "        })\n",
        "\n",
        "        if not satisfied:\n",
        "            self.violations.append({\n",
        "                'timestamp': timestamp,\n",
        "                'cpu_utilization': cpu_util,\n",
        "                'memory_utilization': memory_util,\n",
        "                'cpu_satisfied': cpu_satisfied,\n",
        "                'mem_satisfied': mem_satisfied\n",
        "            })\n",
        "\n",
        "        return satisfied, robustness\n",
        "\n",
        "\n",
        "class STLMonitor:\n",
        "    \"\"\"\n",
        "    Runtime STL Monitor for Microservices Security\n",
        "    Aggregates multiple STL specifications\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.specifications = {\n",
        "            'response_time': ResponseTimeSpec(threshold=100.0),\n",
        "            'threat_detection': ThreatDetectionSpec(response_window=5.0),\n",
        "            'security_posture': SecurityPostureSpec(min_detection_rate=0.95, max_fp_rate=0.05),\n",
        "            'resource_utilization': ResourceUtilizationSpec(min_util=0.2, max_util=0.9)\n",
        "        }\n",
        "        self.global_violations = []\n",
        "        self.current_step = 0\n",
        "\n",
        "    def monitor(self, signal: Dict, timestamp: float) -> Dict:\n",
        "        \"\"\"\n",
        "        Monitor all STL specifications\n",
        "        Returns: aggregated results\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        all_satisfied = True\n",
        "        min_robustness = float('inf')\n",
        "\n",
        "        for name, spec in self.specifications.items():\n",
        "            satisfied, robustness = spec.evaluate(signal, timestamp)\n",
        "            results[name] = {\n",
        "                'satisfied': satisfied,\n",
        "                'robustness': robustness\n",
        "            }\n",
        "\n",
        "            if not satisfied:\n",
        "                all_satisfied = False\n",
        "\n",
        "            min_robustness = min(min_robustness, robustness)\n",
        "\n",
        "        results['global_satisfaction'] = all_satisfied\n",
        "        results['min_robustness'] = min_robustness\n",
        "\n",
        "        if not all_satisfied:\n",
        "            self.global_violations.append({\n",
        "                'timestamp': timestamp,\n",
        "                'step': self.current_step,\n",
        "                'results': results\n",
        "            })\n",
        "\n",
        "        self.current_step += 1\n",
        "        return results\n",
        "\n",
        "    def get_statistics(self) -> Dict:\n",
        "        \"\"\"Get monitoring statistics\"\"\"\n",
        "        stats = {\n",
        "            'total_steps': self.current_step,\n",
        "            'global_violations': len(self.global_violations),\n",
        "            'violation_rate': len(self.global_violations) / max(self.current_step, 1)\n",
        "        }\n",
        "\n",
        "        for name, spec in self.specifications.items():\n",
        "            stats[f'{name}_violations'] = len(spec.violations)\n",
        "            if spec.satisfaction_history:\n",
        "                avg_robustness = np.mean([\n",
        "                    h['robustness'] for h in spec.satisfaction_history\n",
        "                ])\n",
        "                stats[f'{name}_avg_robustness'] = avg_robustness\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset monitor state\"\"\"\n",
        "        for spec in self.specifications.values():\n",
        "            spec.violations = []\n",
        "            spec.satisfaction_history = []\n",
        "        self.global_violations = []\n",
        "        self.current_step = 0\n",
        "\n",
        "\n",
        "print(\"STL Framework implemented successfully!\")\n",
        "print(\"\\nAvailable STL Specifications:\")\n",
        "print(\"  1. ResponseTimeSpec - Response time constraints\")\n",
        "print(\"  2. ThreatDetectionSpec - Threat response timing\")\n",
        "print(\"  3. SecurityPostureSpec - Detection rate and FP rate\")\n",
        "print(\"  4. ResourceUtilizationSpec - Resource usage bounds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bft_section"
      },
      "source": [
        "## 3. Byzantine Fault-Tolerant (BFT) Consensus Protocol\n",
        "### For Multi-Agent Coordination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bft_consensus",
        "outputId": "ec0a3e05-e22a-465f-9ba3-fbb1c3f12d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BFT Consensus Protocol implemented successfully!\n",
            "\n",
            "BFT Features:\n",
            "  Digital signatures for message authentication\n",
            "  Quorum-based voting (2f + 1)\n",
            "  Tolerates up to f Byzantine agents where f < n/3\n",
            "  Three-phase protocol: Prepare Ã¢â€ â€™ Vote Ã¢â€ â€™ Commit\n"
          ]
        }
      ],
      "source": [
        "class BFTAgent:\n",
        "    \"\"\"\n",
        "    Byzantine Fault-Tolerant Agent\n",
        "    Based on Gogada et al. (2023) HotStuff-inspired protocol\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: int, total_agents: int):\n",
        "        self.agent_id = agent_id\n",
        "        self.total_agents = total_agents\n",
        "        self.f = (total_agents - 1) // 3  # Max Byzantine agents (f < n/3)\n",
        "        self.quorum_size = 2 * self.f + 1  # Quorum = 2f + 1\n",
        "\n",
        "        # Generate key pair for digital signatures\n",
        "        self.private_key = rsa.generate_private_key(\n",
        "            public_exponent=65537,\n",
        "            key_size=2048,\n",
        "            backend=default_backend()\n",
        "        )\n",
        "        self.public_key = self.private_key.public_key()\n",
        "\n",
        "        # State\n",
        "        self.view = 0\n",
        "        self.sequence_number = 0\n",
        "        self.message_log = []\n",
        "        self.vote_cache = defaultdict(list)  # proposal_hash -> [votes]\n",
        "\n",
        "    def sign_message(self, message: Dict) -> bytes:\n",
        "        \"\"\"Sign message with private key\"\"\"\n",
        "        message_bytes = json.dumps(message, sort_keys=True).encode()\n",
        "        signature = self.private_key.sign(\n",
        "            message_bytes,\n",
        "            padding.PSS(\n",
        "                mgf=padding.MGF1(hashes.SHA256()),\n",
        "                salt_length=padding.PSS.MAX_LENGTH\n",
        "            ),\n",
        "            hashes.SHA256()\n",
        "        )\n",
        "        return signature\n",
        "\n",
        "    def verify_signature(self, message: Dict, signature: bytes, public_key) -> bool:\n",
        "        \"\"\"Verify message signature\"\"\"\n",
        "        try:\n",
        "            message_bytes = json.dumps(message, sort_keys=True).encode()\n",
        "            public_key.verify(\n",
        "                signature,\n",
        "                message_bytes,\n",
        "                padding.PSS(\n",
        "                    mgf=padding.MGF1(hashes.SHA256()),\n",
        "                    salt_length=padding.PSS.MAX_LENGTH\n",
        "                ),\n",
        "                hashes.SHA256()\n",
        "            )\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def hash_proposal(self, proposal: Dict) -> str:\n",
        "        \"\"\"Create hash of proposal for identification\"\"\"\n",
        "        proposal_str = json.dumps(proposal, sort_keys=True)\n",
        "        return hashlib.sha256(proposal_str.encode()).hexdigest()\n",
        "\n",
        "    def create_proposal(self, action: int, state_info: Dict) -> Dict:\n",
        "        \"\"\"Create action proposal\"\"\"\n",
        "        proposal = {\n",
        "            'agent_id': self.agent_id,\n",
        "            'view': self.view,\n",
        "            'sequence': self.sequence_number,\n",
        "            'action': action,\n",
        "            'state_info': state_info,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "        self.sequence_number += 1\n",
        "        return proposal\n",
        "\n",
        "    def vote_on_proposal(self, proposal: Dict, own_action: int) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Vote on proposal based on agreement with own action\n",
        "        Returns: (vote, reason)\n",
        "        \"\"\"\n",
        "        proposal_action = proposal['action']\n",
        "\n",
        "        # Simple voting logic: agree if actions match or are compatible\n",
        "        if proposal_action == own_action:\n",
        "            return True, \"exact_match\"\n",
        "\n",
        "        # Compatible actions (e.g., Block and Quarantine are both restrictive)\n",
        "        if self._actions_compatible(proposal_action, own_action):\n",
        "            return True, \"compatible\"\n",
        "\n",
        "        return False, \"incompatible\"\n",
        "\n",
        "    def _actions_compatible(self, action1: int, action2: int) -> bool:\n",
        "        \"\"\"Check if two actions are compatible\"\"\"\n",
        "        # Action space: [0: Allow, 1: Block, 2: Quarantine, 3: Alert]\n",
        "        restrictive_actions = {1, 2}  # Block and Quarantine\n",
        "        permissive_actions = {0, 3}  # Allow and Alert\n",
        "\n",
        "        # Both restrictive or both permissive\n",
        "        return (\n",
        "            (action1 in restrictive_actions and action2 in restrictive_actions) or\n",
        "            (action1 in permissive_actions and action2 in permissive_actions)\n",
        "        )\n",
        "\n",
        "\n",
        "class BFTConsensus:\n",
        "    \"\"\"\n",
        "    Byzantine Fault-Tolerant Consensus Manager\n",
        "    Coordinates consensus among multiple agents\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_agents: int):\n",
        "        self.num_agents = num_agents\n",
        "        self.agents = [BFTAgent(i, num_agents) for i in range(num_agents)]\n",
        "        self.f = (num_agents - 1) // 3\n",
        "        self.quorum_size = 2 * self.f + 1\n",
        "\n",
        "        # Statistics\n",
        "        self.consensus_rounds = 0\n",
        "        self.successful_consensus = 0\n",
        "        self.failed_consensus = 0\n",
        "        self.consensus_history = []\n",
        "\n",
        "        print(f\"BFT Consensus initialized:\")\n",
        "        print(f\"  Total agents: {num_agents}\")\n",
        "        print(f\"  Max Byzantine agents (f): {self.f}\")\n",
        "        print(f\"  Quorum size: {self.quorum_size}\")\n",
        "\n",
        "    def reach_consensus(self, actions: List[int], states: torch.Tensor) -> Dict:\n",
        "        \"\"\"\n",
        "        Reach consensus on actions using BFT protocol\n",
        "\n",
        "        Args:\n",
        "            actions: List of proposed actions from each agent\n",
        "            states: Current states\n",
        "\n",
        "        Returns:\n",
        "            consensus_result: Dict with consensus action and metadata\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        self.consensus_rounds += 1\n",
        "\n",
        "        # Phase 1: Prepare - Each agent creates proposal\n",
        "        proposals = []\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            state_info = {\n",
        "                'state': states[i].cpu().tolist() if torch.is_tensor(states[i]) else states[i]\n",
        "            }\n",
        "            proposal = agent.create_proposal(actions[i], state_info)\n",
        "            proposals.append(proposal)\n",
        "\n",
        "        # Phase 2: Vote - Each agent votes on all proposals\n",
        "        vote_matrix = np.zeros((self.num_agents, self.num_agents), dtype=int)\n",
        "\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            for j, proposal in enumerate(proposals):\n",
        "                vote, reason = agent.vote_on_proposal(proposal, actions[i])\n",
        "                vote_matrix[i, j] = 1 if vote else 0\n",
        "\n",
        "        # Phase 3: Commit - Count votes and determine consensus\n",
        "        vote_counts = np.sum(vote_matrix, axis=0)\n",
        "\n",
        "        # Find action(s) with quorum\n",
        "        quorum_reached = vote_counts >= self.quorum_size\n",
        "\n",
        "        if np.any(quorum_reached):\n",
        "            # Consensus reached - use action with most votes\n",
        "            consensus_idx = np.argmax(vote_counts)\n",
        "            consensus_action = actions[consensus_idx]\n",
        "            success = True\n",
        "            self.successful_consensus += 1\n",
        "        else:\n",
        "            # No consensus - use majority vote or default to safe action\n",
        "            action_counts = np.bincount(actions, minlength=4)\n",
        "            consensus_action = np.argmax(action_counts)\n",
        "            success = False\n",
        "            self.failed_consensus += 1\n",
        "\n",
        "        consensus_time = time.time() - start_time\n",
        "\n",
        "        # Record consensus\n",
        "        result = {\n",
        "            'consensus_action': consensus_action,\n",
        "            'success': success,\n",
        "            'vote_counts': vote_counts.tolist(),\n",
        "            'quorum_reached': quorum_reached.tolist(),\n",
        "            'actions': actions,\n",
        "            'consensus_time': consensus_time,\n",
        "            'round': self.consensus_rounds\n",
        "        }\n",
        "\n",
        "        self.consensus_history.append(result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_statistics(self) -> Dict:\n",
        "        \"\"\"Get consensus statistics\"\"\"\n",
        "        if self.consensus_rounds == 0:\n",
        "            return {\n",
        "                'total_rounds': 0,\n",
        "                'success_rate': 0.0,\n",
        "                'avg_consensus_time': 0.0\n",
        "            }\n",
        "\n",
        "        avg_time = np.mean([h['consensus_time'] for h in self.consensus_history])\n",
        "\n",
        "        return {\n",
        "            'total_rounds': self.consensus_rounds,\n",
        "            'successful': self.successful_consensus,\n",
        "            'failed': self.failed_consensus,\n",
        "            'success_rate': self.successful_consensus / self.consensus_rounds,\n",
        "            'avg_consensus_time': avg_time,\n",
        "            'max_byzantine_tolerance': self.f\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"BFT Consensus Protocol implemented successfully!\")\n",
        "print(\"\\nBFT Features:\")\n",
        "print(\"  Digital signatures for message authentication\")\n",
        "print(\"  Quorum-based voting (2f + 1)\")\n",
        "print(\"  Tolerates up to f Byzantine agents where f < n/3\")\n",
        "print(\"  Three-phase protocol: Prepare Ã¢â€ â€™ Vote Ã¢â€ â€™ Commit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "marl_section"
      },
      "source": [
        "## 4. Constraint-Aware MARL Architecture\n",
        "### Using PyTorch/TensorRL with Distributed Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "marl_agent",
        "outputId": "8d07909c-c00a-4e40-f3f0-75a531658603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constraint-Aware MARL Agent implemented!\n",
            "\n",
            "Agent Architecture:\n",
            "  Feature extraction with LayerNorm\n",
            "  LSTM for temporal dependencies\n",
            "  Actor-Critic framework\n",
            "  Constraint satisfaction prediction head\n"
          ]
        }
      ],
      "source": [
        "class ConstraintAwareSecurityAgent(nn.Module):\n",
        "    \"\"\"\n",
        "    Constraint-Aware Security Agent with LSTM\n",
        "    Integrates STL constraints into the reward shaping\n",
        "    Based on Danino et al. (2023) and Prodanov et al. (2025)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        hidden_dim: int = 256,\n",
        "        lstm_layers: int = 2,\n",
        "        dropout: float = 0.2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm_layers = lstm_layers\n",
        "\n",
        "        # Feature extraction network\n",
        "        self.feature_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # LSTM for temporal modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Actor head (policy network)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        # Critic head (value network)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Constraint satisfaction head\n",
        "        self.constraint_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 4),  # 4 STL constraints\n",
        "            nn.Sigmoid()  # Output satisfaction probability\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize network weights using Xavier initialization\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.LSTM):\n",
        "                for name, param in module.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        nn.init.xavier_uniform_(param)\n",
        "                    elif 'bias' in name:\n",
        "                        nn.init.constant_(param, 0)\n",
        "\n",
        "    def forward(self, state, hidden=None, return_constraints=False):\n",
        "        \"\"\"\n",
        "        Forward pass through agent network\n",
        "\n",
        "        Args:\n",
        "            state: Input state tensor [batch_size, state_dim]\n",
        "            hidden: LSTM hidden state (optional)\n",
        "            return_constraints: Whether to return constraint predictions\n",
        "\n",
        "        Returns:\n",
        "            action_logits, state_value, hidden, (constraint_satisfaction)\n",
        "        \"\"\"\n",
        "        # Feature extraction\n",
        "        features = self.feature_net(state)\n",
        "\n",
        "        # Add sequence dimension if needed\n",
        "        if features.dim() == 2:\n",
        "            features = features.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
        "\n",
        "        # LSTM processing\n",
        "        if hidden is None:\n",
        "            lstm_out, hidden = self.lstm(features)\n",
        "        else:\n",
        "            lstm_out, hidden = self.lstm(features, hidden)\n",
        "\n",
        "        lstm_out = lstm_out.squeeze(1)  # [batch_size, hidden_dim]\n",
        "\n",
        "        # Policy and value\n",
        "        action_logits = self.actor(lstm_out)\n",
        "        state_value = self.critic(lstm_out)\n",
        "\n",
        "        if return_constraints:\n",
        "            constraint_satisfaction = self.constraint_net(lstm_out)\n",
        "            return action_logits, state_value, hidden, constraint_satisfaction\n",
        "\n",
        "        return action_logits, state_value, hidden\n",
        "\n",
        "    def get_action(self, state, hidden=None, deterministic=False):\n",
        "        \"\"\"\n",
        "        Sample action from policy\n",
        "\n",
        "        Args:\n",
        "            state: Input state\n",
        "            hidden: LSTM hidden state\n",
        "            deterministic: Whether to use greedy action selection\n",
        "\n",
        "        Returns:\n",
        "            action, log_prob, value, hidden\n",
        "        \"\"\"\n",
        "        action_logits, value, hidden = self.forward(state, hidden)\n",
        "\n",
        "        # Create action distribution\n",
        "        action_probs = F.softmax(action_logits, dim=-1)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        if deterministic:\n",
        "            action = torch.argmax(action_probs, dim=-1)\n",
        "        else:\n",
        "            action = dist.sample()\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        return action, log_prob, value, hidden\n",
        "\n",
        "    def evaluate_actions(self, states, actions, hidden=None):\n",
        "        \"\"\"\n",
        "        Evaluate actions on given states\n",
        "        Used during policy update\n",
        "        \"\"\"\n",
        "        action_logits, values, _, constraints = self.forward(\n",
        "            states, hidden, return_constraints=True\n",
        "        )\n",
        "\n",
        "        action_probs = F.softmax(action_logits, dim=-1)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        log_probs = dist.log_prob(actions)\n",
        "        entropy = dist.entropy()\n",
        "\n",
        "        return values, log_probs, entropy, constraints\n",
        "\n",
        "    def init_hidden(self, batch_size=1, device='cpu'):\n",
        "        \"\"\"Initialize LSTM hidden states\"\"\"\n",
        "        return (\n",
        "            torch.zeros(self.lstm_layers, batch_size, self.hidden_dim).to(device),\n",
        "            torch.zeros(self.lstm_layers, batch_size, self.hidden_dim).to(device)\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"Constraint-Aware MARL Agent implemented!\")\n",
        "print(\"\\nAgent Architecture:\")\n",
        "print(\"  Feature extraction with LayerNorm\")\n",
        "print(\"  LSTM for temporal dependencies\")\n",
        "print(\"  Actor-Critic framework\")\n",
        "print(\"  Constraint satisfaction prediction head\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "marl_env",
        "outputId": "2570f9fb-b0b6-4673-ce99-c50b451792a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Security MARL Environment implemented!\n",
            "\n",
            "Environment Features:\n",
            "  BFT consensus integration\n",
            "  STL monitoring and constraint checking\n",
            "  Multi-agent coordination with cooperation rewards\n",
            "  Comprehensive metrics tracking\n"
          ]
        }
      ],
      "source": [
        "class SecurityMAEnvironment:\n",
        "    \"\"\"\n",
        "    Multi-Agent Security Environment\n",
        "    Integrates STL monitoring and BFT consensus\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_agents: int,\n",
        "        state_dim: int,\n",
        "        use_bft: bool = True,\n",
        "        use_stl: bool = True\n",
        "    ):\n",
        "        self.num_agents = num_agents\n",
        "        self.state_dim = state_dim\n",
        "        self.action_space = 4  # [Allow, Block, Quarantine, Alert]\n",
        "\n",
        "        # Components\n",
        "        self.use_bft = use_bft\n",
        "        self.use_stl = use_stl\n",
        "\n",
        "        if use_bft:\n",
        "            self.bft_consensus = BFTConsensus(num_agents)\n",
        "\n",
        "        if use_stl:\n",
        "            self.stl_monitor = STLMonitor()\n",
        "\n",
        "        # State\n",
        "        self.current_step = 0\n",
        "        self.max_steps = 1000\n",
        "\n",
        "        # Metrics\n",
        "        self.episode_metrics = {\n",
        "            'total_reward': 0,\n",
        "            'correct_detections': 0,\n",
        "            'false_positives': 0,\n",
        "            'false_negatives': 0,\n",
        "            'true_negatives': 0,\n",
        "            'stl_violations': 0,\n",
        "            'consensus_successes': 0\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment\"\"\"\n",
        "        self.current_step = 0\n",
        "        self.episode_metrics = {\n",
        "            'total_reward': 0,\n",
        "            'correct_detections': 0,\n",
        "            'false_positives': 0,\n",
        "            'false_negatives': 0,\n",
        "            'true_negatives': 0,\n",
        "            'stl_violations': 0,\n",
        "            'consensus_successes': 0\n",
        "        }\n",
        "        '''\n",
        "        if self.use_stl:\n",
        "            self.stl_monitor.reset()\n",
        "\n",
        "        return self._get_initial_states()\n",
        "        '''\n",
        "        return torch.zeros(7)\n",
        "\n",
        "    def _get_initial_states(self):\n",
        "        \"\"\"Get initial states for all agents\"\"\"\n",
        "        return torch.zeros(self.num_agents, self.state_dim)\n",
        "\n",
        "    def step(self, actions: List[int], states: torch.Tensor, labels: torch.Tensor = None):\n",
        "        \"\"\"\n",
        "        Execute environment step\n",
        "\n",
        "        Args:\n",
        "            actions: List of actions from each agent\n",
        "            states: Current states\n",
        "            labels: Ground truth labels (for training)\n",
        "\n",
        "        Returns:\n",
        "            next_states, rewards, done, info\n",
        "        \"\"\"\n",
        "        # BFT Consensus (if enabled)\n",
        "        if self.use_bft:\n",
        "            consensus_result = self.bft_consensus.reach_consensus(actions, states)\n",
        "            final_actions = [consensus_result['consensus_action']] * self.num_agents\n",
        "\n",
        "            if consensus_result['success']:\n",
        "                self.episode_metrics['consensus_successes'] += 1\n",
        "        else:\n",
        "            final_actions = actions\n",
        "            consensus_result = None\n",
        "\n",
        "        # Calculate rewards\n",
        "        rewards = []\n",
        "        for i in range(self.num_agents):\n",
        "            reward = self._calculate_reward(\n",
        "                final_actions[i],\n",
        "                states[i],\n",
        "                labels[i] if labels is not None else None\n",
        "            )\n",
        "            rewards.append(reward)\n",
        "\n",
        "        # STL Monitoring (if enabled)\n",
        "        stl_penalty = 0\n",
        "        if self.use_stl:\n",
        "            # Create signal for monitoring\n",
        "            signal = self._create_stl_signal(\n",
        "                final_actions, states, labels, rewards\n",
        "            )\n",
        "\n",
        "            stl_results = self.stl_monitor.monitor(signal, self.current_step)\n",
        "\n",
        "            if not stl_results['global_satisfaction']:\n",
        "                self.episode_metrics['stl_violations'] += 1\n",
        "                # Penalty for STL violations\n",
        "                stl_penalty = -5.0 * (1.0 - stl_results['min_robustness'])\n",
        "\n",
        "        # Add STL penalty to rewards\n",
        "        rewards = [r + stl_penalty for r in rewards]\n",
        "\n",
        "        # Add global reward component (cooperation)\n",
        "        global_reward = self._calculate_global_reward(final_actions, states, labels)\n",
        "        rewards = [r + 0.2 * global_reward for r in rewards]\n",
        "\n",
        "        # Update episode metrics\n",
        "        self.episode_metrics['total_reward'] += sum(rewards)\n",
        "\n",
        "        # Get next states (simplified - just add noise)\n",
        "        next_states = states + torch.randn_like(states) * 0.05\n",
        "\n",
        "        # Check termination\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "\n",
        "        # Info\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'consensus': consensus_result,\n",
        "            'stl_violations': self.episode_metrics['stl_violations'],\n",
        "            'final_actions': final_actions\n",
        "        }\n",
        "\n",
        "        return next_states, rewards, done, info\n",
        "\n",
        "    def _calculate_reward(\n",
        "        self,\n",
        "        action: int,\n",
        "        state: torch.Tensor,\n",
        "        label: Optional[torch.Tensor] = None\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Calculate reward for action\n",
        "        Based on MARLISE reward structure (Prodanov et al., 2025)\n",
        "        \"\"\"\n",
        "        reward = 0.0\n",
        "\n",
        "        if label is not None:\n",
        "            true_label = label.item()\n",
        "\n",
        "            # True Positive: Correctly blocked threat\n",
        "            if action in [1, 2] and true_label == 1:\n",
        "                reward += 15.0\n",
        "                self.episode_metrics['correct_detections'] += 1\n",
        "\n",
        "            # True Negative: Correctly allowed safe activity\n",
        "            elif action in [0, 3] and true_label == 0:\n",
        "                reward += 5.0\n",
        "                self.episode_metrics['true_negatives'] += 1\n",
        "\n",
        "            # False Positive: Blocked safe activity\n",
        "            elif action in [1, 2] and true_label == 0:\n",
        "                reward -= 10.0\n",
        "                self.episode_metrics['false_positives'] += 1\n",
        "\n",
        "            # False Negative: Allowed threat\n",
        "            elif action in [0, 3] and true_label == 1:\n",
        "                reward -= 20.0\n",
        "                self.episode_metrics['false_negatives'] += 1\n",
        "\n",
        "        # Add small negative reward for resource-intensive actions\n",
        "        if action in [1, 2]:  # Block or Quarantine\n",
        "            reward -= 0.5\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def _calculate_global_reward(\n",
        "        self,\n",
        "        actions: List[int],\n",
        "        states: torch.Tensor,\n",
        "        labels: Optional[torch.Tensor] = None\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Calculate global reward for cooperation\n",
        "        Based on LGTC-IPPO (Marino et al., 2025)\n",
        "        \"\"\"\n",
        "        # Consensus bonus\n",
        "        unique_actions = len(set(actions))\n",
        "        consensus_bonus = 5.0 / unique_actions  # Higher reward for agreement\n",
        "\n",
        "        # System-wide security posture\n",
        "        if labels is not None:\n",
        "            correct = sum(1 for a, l in zip(actions, labels)\n",
        "                         if (a in [1, 2] and l == 1) or (a in [0, 3] and l == 0))\n",
        "            accuracy = correct / len(actions)\n",
        "            security_bonus = accuracy * 10.0\n",
        "        else:\n",
        "            security_bonus = 0.0\n",
        "\n",
        "        return consensus_bonus + security_bonus\n",
        "\n",
        "    def _create_stl_signal(self, actions, states, labels, rewards):\n",
        "        \"\"\"Create signal for STL monitoring\"\"\"\n",
        "        # Simulate response time based on action complexity\n",
        "        avg_response_time = 50.0 + np.random.uniform(-20, 30)\n",
        "        if 2 in actions:  # Quarantine is slower\n",
        "            avg_response_time += 30.0\n",
        "\n",
        "        # Check if any threats were detected and acted upon\n",
        "        threat_detected = False\n",
        "        action_taken = False\n",
        "\n",
        "        if labels is not None:\n",
        "            threat_detected = any(l.item() == 1 for l in labels)\n",
        "            action_taken = any(a in [1, 2] for a in actions)\n",
        "\n",
        "        # Calculate detection metrics\n",
        "        total = len(actions)\n",
        "        if labels is not None:\n",
        "            tp = sum(1 for a, l in zip(actions, labels) if a in [1, 2] and l == 1)\n",
        "            fp = sum(1 for a, l in zip(actions, labels) if a in [1, 2] and l == 0)\n",
        "            fn = sum(1 for a, l in zip(actions, labels) if a in [0, 3] and l == 1)\n",
        "            tn = sum(1 for a, l in zip(actions, labels) if a in [0, 3] and l == 0)\n",
        "\n",
        "            detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 1.0\n",
        "            fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "\n",
        "            prediction = 1 if actions[0] in [1, 2] else 0\n",
        "            true_label = labels[0].item()\n",
        "        else:\n",
        "            detection_rate = 0.95\n",
        "            fp_rate = 0.03\n",
        "            prediction = None\n",
        "            true_label = None\n",
        "\n",
        "        # Resource utilization (simulated)\n",
        "        cpu_util = 0.3 + np.random.uniform(0, 0.4)\n",
        "        mem_util = 0.4 + np.random.uniform(0, 0.3)\n",
        "\n",
        "        signal = {\n",
        "            'response_time': avg_response_time,\n",
        "            'threat_detected': threat_detected,\n",
        "            'action_taken': action_taken,\n",
        "            'detection_rate': detection_rate,\n",
        "            'fp_rate': fp_rate,\n",
        "            'prediction': prediction,\n",
        "            'true_label': true_label,\n",
        "            'cpu_utilization': cpu_util,\n",
        "            'memory_utilization': mem_util\n",
        "        }\n",
        "\n",
        "        return signal\n",
        "\n",
        "    def get_metrics(self) -> Dict:\n",
        "        \"\"\"Get episode metrics\"\"\"\n",
        "        total_predictions = (\n",
        "            self.episode_metrics['correct_detections'] +\n",
        "            self.episode_metrics['false_positives'] +\n",
        "            self.episode_metrics['false_negatives'] +\n",
        "            self.episode_metrics['true_negatives']\n",
        "        )\n",
        "\n",
        "        if total_predictions > 0:\n",
        "            accuracy = (\n",
        "                (self.episode_metrics['correct_detections'] +\n",
        "                 self.episode_metrics['true_negatives']) / total_predictions\n",
        "            )\n",
        "\n",
        "            tp = self.episode_metrics['correct_detections']\n",
        "            fp = self.episode_metrics['false_positives']\n",
        "            fn = self.episode_metrics['false_negatives']\n",
        "\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        else:\n",
        "            accuracy = precision = recall = f1 = 0\n",
        "\n",
        "        metrics = {\n",
        "            **self.episode_metrics,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "        if self.use_bft:\n",
        "            metrics['bft_stats'] = self.bft_consensus.get_statistics()\n",
        "\n",
        "        if self.use_stl:\n",
        "            metrics['stl_stats'] = self.stl_monitor.get_statistics()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "print(\"Security MARL Environment implemented!\")\n",
        "print(\"\\nEnvironment Features:\")\n",
        "print(\"  BFT consensus integration\")\n",
        "print(\"  STL monitoring and constraint checking\")\n",
        "print(\"  Multi-agent coordination with cooperation rewards\")\n",
        "print(\"  Comprehensive metrics tracking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trainer_section"
      },
      "source": [
        "## 5. MARL Training Pipeline\n",
        "### IPPO with Constraint Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trainer_code",
        "outputId": "b2708e9c-388c-47cb-a9f8-78554f7e6b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MARL Trainer implemented successfully!\n",
            "\n",
            "Trainer Features:\n",
            "  Independent PPO (IPPO) algorithm\n",
            "  Generalized Advantage Estimation (GAE)\n",
            "  Constraint-aware loss function\n",
            "  BFT consensus and STL monitoring integration\n",
            "  Comprehensive evaluation metrics\n"
          ]
        }
      ],
      "source": [
        "class ConstraintAwareMARLTrainer:\n",
        "    \"\"\"\n",
        "    Multi-Agent RL Trainer with STL Constraints and BFT Consensus\n",
        "    Uses Independent PPO (IPPO) algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_agents: int,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        hidden_dim: int = 256,\n",
        "        lr: float = 3e-4,\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        clip_epsilon: float = 0.2,\n",
        "        value_coef: float = 0.5,\n",
        "        entropy_coef: float = 0.01,\n",
        "        constraint_coef: float = 0.1,\n",
        "        use_bft: bool = True,\n",
        "        use_stl: bool = True,\n",
        "        device: str = 'cuda'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_agents = num_agents\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.clip_epsilon = clip_epsilon\n",
        "        self.value_coef = value_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.constraint_coef = constraint_coef\n",
        "\n",
        "        # Create agents\n",
        "        self.agents = [\n",
        "            ConstraintAwareSecurityAgent(\n",
        "                state_dim, action_dim, hidden_dim\n",
        "            ).to(self.device)\n",
        "            for _ in range(num_agents)\n",
        "        ]\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizers = [\n",
        "            optim.Adam(agent.parameters(), lr=lr)\n",
        "            for agent in self.agents\n",
        "        ]\n",
        "\n",
        "        # Environment\n",
        "        self.env = SecurityMAEnvironment(\n",
        "            num_agents, state_dim, use_bft=use_bft, use_stl=use_stl\n",
        "        )\n",
        "\n",
        "        # Training statistics\n",
        "        self.training_stats = {\n",
        "            'episode_rewards': [],\n",
        "            'policy_losses': [],\n",
        "            'value_losses': [],\n",
        "            'constraint_losses': [],\n",
        "            'stl_violations': [],\n",
        "            'consensus_success_rate': [],\n",
        "            'detection_accuracy': [],\n",
        "            'precision': [],\n",
        "            'recall': [],\n",
        "            'f1_score': []\n",
        "        }\n",
        "\n",
        "        print(f\"Trainer initialized on {self.device}\")\n",
        "        print(f\"Number of agents: {num_agents}\")\n",
        "        print(f\"BFT enabled: {use_bft}\")\n",
        "        print(f\"STL monitoring enabled: {use_stl}\")\n",
        "\n",
        "    def collect_trajectories(self, data_loader, num_steps):\n",
        "        \"\"\"\n",
        "        Collect trajectories from DARPA dataset\n",
        "        \"\"\"\n",
        "        trajectories = [[] for _ in range(self.num_agents)]\n",
        "        hiddens = [agent.init_hidden(1, self.device) for agent in self.agents]\n",
        "\n",
        "        data_iter = iter(data_loader)\n",
        "        steps = 0\n",
        "\n",
        "        # Reset environment at the start of trajectory collection for a new episode\n",
        "        self.env.reset()\n",
        "\n",
        "        while steps < num_steps:\n",
        "            try:\n",
        "                states_batch, labels_batch, _ = next(data_iter)\n",
        "            except StopIteration:\n",
        "                # Reset data_iter if it runs out of batches\n",
        "                data_iter = iter(data_loader)\n",
        "                states_batch, labels_batch, _ = next(data_iter)\n",
        "\n",
        "            states_batch = states_batch.to(self.device)\n",
        "            labels_batch = labels_batch.to(self.device)\n",
        "\n",
        "            batch_size_data = states_batch.size(0) # Batch size from DataLoader\n",
        "\n",
        "            for b in range(batch_size_data): # Iterate through each sample in the batch\n",
        "                if steps >= num_steps:\n",
        "                    break\n",
        "\n",
        "                # Get state and label for current step from batch\n",
        "                state_single_sample = states_batch[b:b+1]  # [1, state_dim]\n",
        "                label_single_sample = labels_batch[b:b+1] # [1]\n",
        "\n",
        "                # Replicate state for all agents as input to their individual networks\n",
        "                # All agents observe the same state in a centralized training, decentralized execution setup\n",
        "                states_for_env_input = state_single_sample.repeat(self.num_agents, 1) # [num_agents, state_dim]\n",
        "                labels_for_env_input = label_single_sample.repeat(self.num_agents)   # [num_agents]\n",
        "\n",
        "                # Get actions from all agents\n",
        "                actions = []\n",
        "                log_probs = []\n",
        "                values = []\n",
        "\n",
        "                for i, agent in enumerate(self.agents):\n",
        "                    # Each agent processes the single state sample through its full network\n",
        "                    action, log_prob, value, hiddens[i] = agent.get_action(\n",
        "                        state_single_sample, hiddens[i], deterministic=False\n",
        "                    )\n",
        "                    actions.append(action.item())\n",
        "                    log_probs.append(log_prob)\n",
        "                    values.append(value)\n",
        "\n",
        "                # Environment step with collected actions\n",
        "                next_states, rewards, done, info = self.env.step(\n",
        "                    actions, states_for_env_input, labels_for_env_input\n",
        "                )\n",
        "\n",
        "                # Store transitions for each agent\n",
        "                for i in range(self.num_agents):\n",
        "                    # Store the state that *was observed* by the agent at this step (state_single_sample)\n",
        "                    trajectories[i].append({\n",
        "                        'state': state_single_sample.squeeze(0).cpu(), # [state_dim]\n",
        "                        'action': actions[i],\n",
        "                        'reward': rewards[i],\n",
        "                        'log_prob': log_probs[i].detach().cpu(),\n",
        "                        'value': values[i].detach().cpu(),\n",
        "                        'done': done # done state of the environment\n",
        "                    })\n",
        "\n",
        "                steps += 1\n",
        "\n",
        "                # If the environment signals done, reset for the next episode\n",
        "                if done:\n",
        "                    self.env.reset()\n",
        "                    # Reset LSTM hidden states for the new episode\n",
        "                    hiddens = [agent.init_hidden(1, self.device) for agent in self.agents]\n",
        "\n",
        "        return trajectories\n",
        "\n",
        "    def compute_gae(self, trajectories):\n",
        "        \"\"\"\n",
        "        Compute Generalized Advantage Estimation\n",
        "        \"\"\"\n",
        "        for agent_traj in trajectories:\n",
        "            advantages = []\n",
        "            returns = []\n",
        "\n",
        "            gae = 0\n",
        "            next_value = 0\n",
        "\n",
        "            for t in reversed(range(len(agent_traj))):\n",
        "                reward = agent_traj[t]['reward']\n",
        "                value = agent_traj[t]['value'].item()\n",
        "                done = agent_traj[t]['done']\n",
        "\n",
        "                # TD error\n",
        "                delta = reward + self.gamma * next_value * (1 - done) - value\n",
        "\n",
        "                # GAE\n",
        "                gae = delta + self.gamma * self.gae_lambda * (1 - done) * gae\n",
        "\n",
        "                advantages.insert(0, gae)\n",
        "                returns.insert(0, gae + value)\n",
        "\n",
        "                next_value = value\n",
        "\n",
        "            # Store computed values\n",
        "            for t in range(len(agent_traj)):\n",
        "                agent_traj[t]['advantage'] = advantages[t]\n",
        "                agent_traj[t]['return'] = returns[t]\n",
        "\n",
        "        return trajectories\n",
        "\n",
        "    def update_policy(self, trajectories, epochs):\n",
        "        \"\"\"\n",
        "        Update policies using PPO\n",
        "        \"\"\"\n",
        "        policy_losses = []\n",
        "        value_losses = []\n",
        "        constraint_losses = []\n",
        "\n",
        "        for agent_idx in range(self.num_agents):\n",
        "            agent = self.agents[agent_idx]\n",
        "            optimizer = self.optimizers[agent_idx]\n",
        "            agent_traj = trajectories[agent_idx]\n",
        "\n",
        "            if len(agent_traj) == 0:\n",
        "                continue\n",
        "\n",
        "            # Prepare batch data\n",
        "            states = torch.stack([t['state'] for t in agent_traj]).to(self.device)\n",
        "            actions = torch.tensor(\n",
        "                [t['action'] for t in agent_traj], dtype=torch.long\n",
        "            ).to(self.device)\n",
        "            old_log_probs = torch.stack(\n",
        "                [t['log_prob'] for t in agent_traj]\n",
        "            ).to(self.device)\n",
        "            advantages = torch.tensor(\n",
        "                [t['advantage'] for t in agent_traj], dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "            returns = torch.tensor(\n",
        "                [t['return'] for t in agent_traj], dtype=torch.float32\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Normalize advantages\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "            # PPO update epochs\n",
        "            for epoch in range(epochs):\n",
        "                # Forward pass\n",
        "                values, log_probs, entropy, constraints = agent.evaluate_actions(\n",
        "                    states, actions\n",
        "                )\n",
        "\n",
        "                # Ratio for PPO\n",
        "                ratio = torch.exp(log_probs - old_log_probs.detach())\n",
        "\n",
        "                # Clipped surrogate objective\n",
        "                surr1 = ratio * advantages\n",
        "                surr2 = torch.clamp(\n",
        "                    ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon\n",
        "                ) * advantages\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                value_loss = F.mse_loss(values.squeeze(), returns)\n",
        "\n",
        "                # Entropy bonus\n",
        "                entropy_loss = -entropy.mean()\n",
        "\n",
        "                # Constraint loss (encourage constraint satisfaction)\n",
        "                # Target: all constraints should be satisfied (1.0)\n",
        "                constraint_target = torch.ones_like(constraints)\n",
        "                constraint_loss = F.mse_loss(constraints, constraint_target)\n",
        "\n",
        "                # Total loss\n",
        "                total_loss = (\n",
        "                    policy_loss +\n",
        "                    self.value_coef * value_loss +\n",
        "                    self.entropy_coef * entropy_loss +\n",
        "                    self.constraint_coef * constraint_loss\n",
        "                )\n",
        "\n",
        "                # Backprop\n",
        "                optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
        "                optimizer.step()\n",
        "\n",
        "                policy_losses.append(policy_loss.item())\n",
        "                value_losses.append(value_loss.item())\n",
        "                constraint_losses.append(constraint_loss.item())\n",
        "\n",
        "        return (\n",
        "            np.mean(policy_losses) if policy_losses else 0,\n",
        "            np.mean(value_losses) if value_losses else 0,\n",
        "            np.mean(constraint_losses) if constraint_losses else 0\n",
        "        )\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        num_iterations: int = 20,\n",
        "        eval_interval: int = 5,\n",
        "        save_interval: int = 5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Main training loop\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"STARTING MARL TRAINING\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        best_f1 = 0.0\n",
        "\n",
        "        for iteration in range(num_iterations):\n",
        "\n",
        "            print(f\"\\n[Iteration {iteration+1}/{num_iterations}] Starting...\")\n",
        "            start_time = time.time()\n",
        "            # Collect trajectories\n",
        "            print(f\"  Collecting trajectories...\")\n",
        "            # num_steps argument is passed to collect_trajectories\n",
        "            trajectories = self.collect_trajectories(train_loader, num_steps=128) #2048, 512\n",
        "            print(f\"  âœ“ Collected in {time.time()-start_time:.1f}s\")\n",
        "\n",
        "            # Compute advantages\n",
        "            trajectories = self.compute_gae(trajectories)\n",
        "\n",
        "            # Update policies\n",
        "            policy_loss, value_loss, constraint_loss = self.update_policy(\n",
        "                trajectories, epochs=2) #epochs=10\n",
        "\n",
        "            # Get episode metrics\n",
        "            metrics = self.env.get_metrics()\n",
        "\n",
        "            # Log statistics\n",
        "            self.training_stats['episode_rewards'].append(metrics['total_reward'])\n",
        "            self.training_stats['policy_losses'].append(policy_loss)\n",
        "            self.training_stats['value_losses'].append(value_loss)\n",
        "            self.training_stats['constraint_losses'].append(constraint_loss)\n",
        "            self.training_stats['stl_violations'].append(metrics['stl_violations'])\n",
        "            self.training_stats['detection_accuracy'].append(metrics['accuracy'])\n",
        "            self.training_stats['precision'].append(metrics['precision'])\n",
        "            self.training_stats['recall'].append(metrics['recall'])\n",
        "            self.training_stats['f1_score'].append(metrics['f1'])\n",
        "\n",
        "            if 'bft_stats' in metrics:\n",
        "                self.training_stats['consensus_success_rate'].append(\n",
        "                    metrics['bft_stats']['success_rate']\n",
        "                )\n",
        "\n",
        "            # Print progress\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                print(f\"\\nIteration {iteration + 1}/{num_iterations}\")\n",
        "                print(f\"  Reward: {metrics['total_reward']:.2f}\")\n",
        "                print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "                print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
        "                print(f\"  STL Violations: {metrics['stl_violations']}\")\n",
        "                print(f\"  Policy Loss: {policy_loss:.4f}\")\n",
        "                print(f\"  Value Loss: {value_loss:.4f}\")\n",
        "                print(f\"  Constraint Loss: {constraint_loss:.4f}\")\n",
        "\n",
        "                if 'bft_stats' in metrics:\n",
        "                    print(f\"  BFT Success Rate: {metrics['bft_stats']['success_rate']:.2%}\")\n",
        "\n",
        "            # Evaluation\n",
        "            if (iteration + 1) % eval_interval == 0:\n",
        "                val_metrics = self.evaluate(val_loader)\n",
        "\n",
        "                print(f\"\\n{'='*80}\")\n",
        "                print(f\"EVALUATION at iteration {iteration + 1}\")\n",
        "                print(f\"{'='*80}\")\n",
        "                print(f\"  Accuracy:  {val_metrics['accuracy']:.4f}\")\n",
        "                print(f\"  Precision: {val_metrics['precision']:.4f}\")\n",
        "                print(f\"  Recall:    {val_metrics['recall']:.4f}\")\n",
        "                print(f\"  F1-Score:  {val_metrics['f1']:.4f}\")\n",
        "                print(f\"  STL Violations: {val_metrics.get('stl_violations', 0)}\")\n",
        "                print(f\"{'='*80}\\n\")\n",
        "\n",
        "                # Save best model\n",
        "                if val_metrics['f1'] > best_f1:\n",
        "                    best_f1 = val_metrics['f1']\n",
        "                    self.save_models('models/best_marl_agents')\n",
        "                    print(f\"\\u2713 New best model saved! F1-Score: {best_f1:.4f}\")\n",
        "\n",
        "            # Reset environment for next iteration\n",
        "            self.env.reset()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"TRAINING COMPLETED!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return self.training_stats\n",
        "\n",
        "    def evaluate(self, data_loader, num_batches: int = 10):\n",
        "        \"\"\"\n",
        "        Evaluate agents on validation/test set\n",
        "        \"\"\"\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        # Set agents to eval mode\n",
        "        for agent in self.agents:\n",
        "            agent.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (states, labels, _) in enumerate(data_loader):\n",
        "                if batch_idx >= num_batches:\n",
        "                    break\n",
        "\n",
        "                states = states.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                batch_size = states.size(0)\n",
        "\n",
        "                # Get predictions from all agents\n",
        "                agent_predictions = []\n",
        "\n",
        "                for agent in self.agents:\n",
        "                    # Evaluate with agent's full forward pass (not direct actor call)\n",
        "                    action_logits, _, _ = agent(states)\n",
        "                    action_probs = F.softmax(action_logits, dim=-1)\n",
        "                    # Convert to binary: restrictive (1,2) vs permissive (0,3)\n",
        "                    actions = torch.argmax(action_probs, dim=-1)\n",
        "                    predictions = (actions >= 1).long()  # Block/Quarantine = 1\n",
        "                    agent_predictions.append(predictions)\n",
        "\n",
        "                # Majority voting\n",
        "                agent_predictions = torch.stack(agent_predictions)\n",
        "                final_predictions = (agent_predictions.float().mean(0) >= 0.5).long()\n",
        "\n",
        "                all_predictions.extend(final_predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Set agents back to train mode\n",
        "        for agent in self.agents:\n",
        "            agent.train()\n",
        "\n",
        "        # Calculate metrics\n",
        "        all_predictions = np.array(all_predictions)\n",
        "        all_labels = np.array(all_labels)\n",
        "\n",
        "        tp = np.sum((all_predictions == 1) & (all_labels == 1))\n",
        "        tn = np.sum((all_predictions == 0) & (all_labels == 0))\n",
        "        fp = np.sum((all_predictions == 1) & (all_labels == 0))\n",
        "        fn = np.sum((all_predictions == 0) & (all_labels == 1))\n",
        "\n",
        "        accuracy = (tp + tn) / len(all_labels) if len(all_labels) > 0 else 0\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'tp': int(tp),\n",
        "            'tn': int(tn),\n",
        "            'fp': int(fp),\n",
        "            'fn': int(fn)\n",
        "        }\n",
        "\n",
        "    def save_models(self, path: str = 'models/marl_agents'):\n",
        "        \"\"\"\n",
        "        Save trained agent models\n",
        "        \"\"\"\n",
        "        import os\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            torch.save(agent.state_dict(), f'{path}/agent_{i}.pth')\n",
        "\n",
        "        # Save training stats\n",
        "        with open(f'{path}/training_stats.json', 'w') as f:\n",
        "            # Convert numpy types to Python types for JSON\n",
        "            stats_json = {}\n",
        "            for key, value in self.training_stats.items():\n",
        "                if isinstance(value, list):\n",
        "                    stats_json[key] = [float(v) if isinstance(v, (np.floating, float)) else int(v)\n",
        "                                      for v in value]\n",
        "            json.dump(stats_json, f, indent=2)\n",
        "\n",
        "        print(f\"\\u2713 Models and stats saved to {path}\")\n",
        "\n",
        "    def load_models(self, path: str = 'models/marl_agents'):\n",
        "        \"\"\"\n",
        "        Load trained agent models\n",
        "        \"\"\"\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            agent.load_state_dict(torch.load(f'{path}/agent_{i}.pth'))\n",
        "        print(f\"\\u2713 Models loaded from {path}\")\n",
        "\n",
        "\n",
        "print(\"MARL Trainer implemented successfully!\")\n",
        "print(\"\\nTrainer Features:\")\n",
        "print(\"  Independent PPO (IPPO) algorithm\")\n",
        "print(\"  Generalized Advantage Estimation (GAE)\")\n",
        "print(\"  Constraint-aware loss function\")\n",
        "print(\"  BFT consensus and STL monitoring integration\")\n",
        "print(\"  Comprehensive evaluation metrics\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CORRECTED update_agent_t4 Function\n",
        "Replace the existing function in the notebook with this corrected version\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.cuda.amp import autocast\n",
        "import numpy as np\n",
        "\n",
        "def update_agent_t4(agent, experiences, config):\n",
        "    \"\"\"\n",
        "    T4-Optimized agent update with mixed precision training\n",
        "\n",
        "    FIXED: Now properly uses agent.forward() instead of directly calling\n",
        "    agent.actor/agent.critic, which bypassed feature_net and LSTM layers.\n",
        "    \"\"\"\n",
        "    use_amp = config.get('use_mixed_precision', True)\n",
        "    device = next(agent.parameters()).device\n",
        "\n",
        "    # Prepare batches\n",
        "    states = torch.FloatTensor(experiences['states']).to(device)\n",
        "    actions = torch.LongTensor(experiences['actions']).to(device)\n",
        "    old_log_probs = torch.FloatTensor(experiences['log_probs']).to(device)\n",
        "    advantages = torch.FloatTensor(experiences['advantages']).to(device)\n",
        "    returns = torch.FloatTensor(experiences['returns']).to(device)\n",
        "\n",
        "    # Mini-batch training for memory efficiency on T4\n",
        "    batch_size = len(states)\n",
        "    mini_batch_size = config.get('mini_batch_size', 64)\n",
        "    indices = np.arange(batch_size)\n",
        "\n",
        "    policy_losses = []\n",
        "    value_losses = []\n",
        "\n",
        "    for _ in range(4):  # Multiple epochs\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for start in range(0, batch_size, mini_batch_size):\n",
        "            end = min(start + mini_batch_size, batch_size)\n",
        "            batch_indices = indices[start:end]\n",
        "\n",
        "            batch_states = states[batch_indices]\n",
        "            batch_actions = actions[batch_indices]\n",
        "            batch_old_log_probs = old_log_probs[batch_indices]\n",
        "            batch_advantages = advantages[batch_indices]\n",
        "            batch_returns = returns[batch_indices]\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            if use_amp:\n",
        "                with autocast():\n",
        "                    # âœ… FIXED: Use full forward pass through agent architecture\n",
        "                    # This processes: state â†’ feature_net â†’ LSTM â†’ actor/critic\n",
        "                    action_logits, values, _ = agent(batch_states)\n",
        "\n",
        "                    # Get action probabilities and create distribution\n",
        "                    action_probs = F.softmax(action_logits, dim=-1)\n",
        "                    dist = Categorical(action_probs)\n",
        "                    new_log_probs = dist.log_prob(batch_actions)\n",
        "                    entropy = dist.entropy().mean()\n",
        "\n",
        "                    # Values are already computed from forward pass\n",
        "                    values = values.squeeze(-1)\n",
        "\n",
        "                    # PPO loss\n",
        "                    ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
        "                    surr1 = ratio * batch_advantages\n",
        "                    surr2 = torch.clamp(ratio, 1.0 - config['epsilon_clip'],\n",
        "                                       1.0 + config['epsilon_clip']) * batch_advantages\n",
        "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                    policy_loss = policy_loss - config['entropy_coef'] * entropy\n",
        "\n",
        "                    value_loss = F.mse_loss(values, batch_returns)\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                agent.actor_optimizer.zero_grad()\n",
        "                agent.scaler.scale(policy_loss).backward()\n",
        "                agent.scaler.unscale_(agent.actor_optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),\n",
        "                                               config['max_grad_norm'])\n",
        "                agent.scaler.step(agent.actor_optimizer)\n",
        "\n",
        "                agent.critic_optimizer.zero_grad()\n",
        "                agent.scaler.scale(value_loss).backward()\n",
        "                agent.scaler.unscale_(agent.critic_optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(agent.critic.parameters(),\n",
        "                                               config['max_grad_norm'])\n",
        "                agent.scaler.step(agent.critic_optimizer)\n",
        "\n",
        "                agent.scaler.update()\n",
        "            else:\n",
        "                # Standard FP32 training\n",
        "                # âœ… FIXED: Use full forward pass through agent architecture\n",
        "                action_logits, values, _ = agent(batch_states)\n",
        "\n",
        "                # Get action probabilities and create distribution\n",
        "                action_probs = F.softmax(action_logits, dim=-1)\n",
        "                dist = Categorical(action_probs)\n",
        "                new_log_probs = dist.log_prob(batch_actions)\n",
        "                entropy = dist.entropy().mean()\n",
        "\n",
        "                # Values are already computed from forward pass\n",
        "                values = values.squeeze(-1)\n",
        "\n",
        "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
        "                surr1 = ratio * batch_advantages\n",
        "                surr2 = torch.clamp(ratio, 1.0 - config['epsilon_clip'],\n",
        "                                   1.0 + config['epsilon_clip']) * batch_advantages\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                policy_loss = policy_loss - config['entropy_coef'] * entropy\n",
        "\n",
        "                value_loss = F.mse_loss(values, batch_returns)\n",
        "\n",
        "                # Backward\n",
        "                agent.actor_optimizer.zero_grad()\n",
        "                policy_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),\n",
        "                                               config['max_grad_norm'])\n",
        "                agent.actor_optimizer.step()\n",
        "\n",
        "                agent.critic_optimizer.zero_grad()\n",
        "                value_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(agent.critic.parameters(),\n",
        "                                               config['max_grad_norm'])\n",
        "                agent.critic_optimizer.step()\n",
        "\n",
        "            policy_losses.append(policy_loss.item())\n",
        "            value_losses.append(value_loss.item())\n",
        "\n",
        "    return {\n",
        "        'policy_loss': np.mean(policy_losses),\n",
        "        'value_loss': np.mean(value_losses)\n",
        "    }\n",
        "\n",
        "print(\"âœ“ CORRECTED T4-optimized training function loaded\")\n",
        "print(\"  Fixed: Now uses agent.forward() for proper state processing\")\n",
        "print(\"  State flow: Input(7) â†’ Feature_net â†’ LSTM(256) â†’ Actor/Critic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SumUwJlIogFh",
        "outputId": "acb12e5c-e04d-499c-d02f-8fd984267142"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ CORRECTED T4-optimized training function loaded\n",
            "  Fixed: Now uses agent.forward() for proper state processing\n",
            "  State flow: Input(7) â†’ Feature_net â†’ LSTM(256) â†’ Actor/Critic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount_section"
      },
      "source": [
        "## 6. Mount Google Drive and Load Data\n",
        "### Load preprocessed DARPA TC dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mount_drive",
        "outputId": "783621d8-d83c-42d2-b092-24363300204a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted\n",
            "Base directory: /content/drive/MyDrive/mythesis/vicky/darpa_tc\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set base directory (adjust to your path)\n",
        "BASE_DIR = \"/content/drive/MyDrive/mythesis/vicky/darpa_tc\"\n",
        "\n",
        "print(f\"Drive mounted\")\n",
        "print(f\"Base directory: {BASE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_data",
        "outputId": "253517d8-3378-4c3b-fe46-b691b9a0e4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DARPA TC datasets...\n",
            "================================================================================\n",
            "Loading dataset from /content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/train.parquet...\n",
            "Loaded 350,000 records\n",
            "Using 7 features\n",
            "Loading dataset from /content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/val.parquet...\n",
            "Loaded 75,000 records\n",
            "Using 7 features\n",
            "Loading dataset from /content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/test.parquet...\n",
            "Loaded 75,000 records\n",
            "Using 7 features\n",
            "\n",
            "Dataloaders created\n",
            "  Train batches: 10938\n",
            "  Val batches:   2344\n",
            "  Test batches:  2344\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Import your dataset class from the preprocessing notebook\n",
        "class DARPAAPTDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for DARPA TC data\"\"\"\n",
        "\n",
        "    def __init__(self, parquet_file, feature_columns=None):\n",
        "        print(f\"Loading dataset from {parquet_file}...\")\n",
        "        self.df = pd.read_parquet(parquet_file)\n",
        "\n",
        "        if feature_columns is None:\n",
        "            self.feature_columns = [\n",
        "                'sequence', 'src_port', 'dst_port',\n",
        "                'ip_protocol', 'file_size', 'process_pid', 'process_ppid'\n",
        "            ]\n",
        "        else:\n",
        "            self.feature_columns = feature_columns\n",
        "\n",
        "        # Ensure columns exist\n",
        "        for col in self.feature_columns:\n",
        "            if col not in self.df.columns:\n",
        "                self.df[col] = 0\n",
        "\n",
        "        # Fill missing values\n",
        "        for col in self.feature_columns:\n",
        "            self.df[col] = self.df[col].fillna(0)\n",
        "\n",
        "        # Normalize features\n",
        "        self.normalize_features()\n",
        "\n",
        "        print(f\"Loaded {len(self.df):,} records\")\n",
        "        print(f\"Using {len(self.feature_columns)} features\")\n",
        "\n",
        "    def normalize_features(self):\n",
        "        \"\"\"Normalize numeric features to [0, 1] range\"\"\"\n",
        "        for col in self.feature_columns:\n",
        "            max_val = self.df[col].max()\n",
        "            if max_val > 0:\n",
        "                self.df[col] = self.df[col] / max_val\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        features = []\n",
        "        for col in self.feature_columns:\n",
        "            features.append(float(row[col]))\n",
        "\n",
        "        state = torch.tensor(features, dtype=torch.float32)\n",
        "        label = torch.tensor(int(row['is_suspicious']), dtype=torch.long)\n",
        "\n",
        "        metadata = {\n",
        "            'record_type': row.get('record_type', ''),\n",
        "            'timestamp': str(row.get('timestamp', ''))\n",
        "        }\n",
        "\n",
        "        return state, label, metadata\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading DARPA TC datasets...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_dataset = DARPAAPTDataset(f\"{BASE_DIR}/splits/train.parquet\")\n",
        "val_dataset = DARPAAPTDataset(f\"{BASE_DIR}/splits/val.parquet\")\n",
        "test_dataset = DARPAAPTDataset(f\"{BASE_DIR}/splits/test.parquet\")\n",
        "'''\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # CRITICAL: Must be 0 in Colab\n",
        "    pin_memory=False,  # CHANGED: Disable pin_memory\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0,  # CRITICAL: Must be 0\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0,  # CRITICAL: Must be 0\n",
        "    pin_memory=False\n",
        ")\n",
        "print(\"\\nDataloaders created\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches:   {len(val_loader)}\")\n",
        "print(f\"  Test batches:  {len(test_loader)}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section"
      },
      "source": [
        "## 7. Initialize and Train MARL System"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRECTED T4-Optimized Configuration\n",
        "# Fix: Updated state_dim from 64 to 7 to match actual dataset features\n",
        "\n",
        "config = {\n",
        "    # Environment\n",
        "    'n_agents': 2,\n",
        "    'state_dim': 7,  # âœ… FIXED: Changed from 64 to 7 (matches DARPA dataset features)\n",
        "    'action_dim': 5,\n",
        "    'use_stl': False,\n",
        "    'use_bft': False,\n",
        "\n",
        "    # T4-Optimized Training Parameters\n",
        "    'n_iterations': 20,\n",
        "    'n_episodes_per_iter': 3,\n",
        "    'max_steps_per_episode': 30,\n",
        "    'batch_size': 128,\n",
        "    'mini_batch_size': 64,\n",
        "\n",
        "    # Mixed Precision Training (T4 Feature)\n",
        "    'use_mixed_precision': True,\n",
        "    'fp16_opt_level': 'O1',\n",
        "\n",
        "    # Learning\n",
        "    'lr_actor': 3e-4,\n",
        "    'lr_critic': 1e-3,\n",
        "    'gamma': 0.99,\n",
        "    'gae_lambda': 0.95,\n",
        "    'epsilon_clip': 0.2,\n",
        "    'value_loss_coef': 0.5,\n",
        "    'entropy_coef': 0.01,\n",
        "    'max_grad_norm': 0.5,\n",
        "\n",
        "    # Constraint Learning\n",
        "    'constraint_coef': 0.3,\n",
        "    'stl_penalty': 1.0,\n",
        "\n",
        "    # Memory Optimization for T4\n",
        "    'gradient_checkpointing': False,\n",
        "    'pin_memory': True,\n",
        "    'num_workers': 2,\n",
        "\n",
        "    # Network Architecture (T4-efficient)\n",
        "    'hidden_dim': 256,\n",
        "    'lstm_layers': 2,\n",
        "    'lstm_hidden': 64,\n",
        "\n",
        "    # Replay Buffer (T4 memory-aware)\n",
        "    'buffer_size': 50000,\n",
        "\n",
        "    # Evaluation\n",
        "    'eval_frequency': 10,\n",
        "    'n_eval_episodes': 10,\n",
        "\n",
        "    # Logging\n",
        "    'log_frequency': 10,\n",
        "    'save_frequency': 50,\n",
        "\n",
        "    # DARPA Dataset\n",
        "    'dataset_path': '/content/darpa_tc',\n",
        "    'apt_scenarios': ['trace', 'theia', 'fivedirections'],\n",
        "\n",
        "    # STL Specifications\n",
        "    'response_time_threshold': 100.0,\n",
        "    'detection_time_threshold': 50.0,\n",
        "    'min_detection_rate': 0.95,\n",
        "    'max_false_positive_rate': 0.05,\n",
        "\n",
        "    # BFT Parameters\n",
        "    'bft_quorum_size': 2,\n",
        "    'max_byzantine_agents': 1,\n",
        "}\n",
        "\n",
        "# Validation: Ensure config matches implementation\n",
        "print(\"âœ“ Configuration Validated:\")\n",
        "print(f\"  Dataset features: 7\")\n",
        "print(f\"  Config state_dim: {config['state_dim']}\")\n",
        "print(f\"  Match: {'âœ…' if config['state_dim'] == 7 else 'âŒ'}\")\n",
        "print(f\"\\nNetwork Architecture Flow:\")\n",
        "print(f\"  Input: (batch, {config['state_dim']}) â†’ Feature extraction\")\n",
        "print(f\"  Hidden: (batch, {config['hidden_dim']}) â†’ LSTM processing\")\n",
        "print(f\"  Output: (batch, {config['action_dim']}) â†’ Actions\")\n",
        "\n",
        "\n",
        "# Create base directory\n",
        "BASE_DIR = '/content/drive/MyDrive/mythesis/vicky/marl_stl_bft_results'\n",
        "Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(f'{BASE_DIR}/models').mkdir(exist_ok=True)\n",
        "\n",
        "# Save configuration\n",
        "with open(f'{BASE_DIR}/config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"âœ“ T4-Optimized Configuration loaded\")\n",
        "print(f\"\\nKey T4 Settings:\")\n",
        "print(f\"  Batch Size: {config['batch_size']}\")\n",
        "print(f\"  Mixed Precision: {config['use_mixed_precision']}\")\n",
        "print(f\"  Hidden Dim: {config['hidden_dim']}\")\n",
        "print(f\"  Buffer Size: {config['buffer_size']:,}\")\n",
        "print(f\"  Pin Memory: {config['pin_memory']}\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = ConstraintAwareMARLTrainer(\n",
        "    num_agents=config['n_agents'],\n",
        "    state_dim= 7, #train_dataset.feature_columns.__len__(), # Use actual state_dim from dataset\n",
        "    action_dim=config['action_dim'],\n",
        "    hidden_dim=config['hidden_dim'],\n",
        "    lr=config['lr_actor'], # Using actor LR for simplicity, could use separate\n",
        "    gamma=config['gamma'],\n",
        "    gae_lambda=config['gae_lambda'],\n",
        "    clip_epsilon=config['epsilon_clip'],\n",
        "    value_coef=config['value_loss_coef'],\n",
        "    entropy_coef=config['entropy_coef'],\n",
        "    constraint_coef=config['constraint_coef'],\n",
        "    use_bft=config['use_bft'],\n",
        "    use_stl=config['use_stl'],\n",
        "    device=device # Use the 'device' variable defined earlier\n",
        ")\n",
        "print(\"\\nâœ“ MARL Trainer initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuyLXjyZrdtK",
        "outputId": "bb7c47ea-2d82-461d-ee27-2c807a1e622d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Configuration Validated:\n",
            "  Dataset features: 7\n",
            "  Config state_dim: 7\n",
            "  Match: âœ…\n",
            "\n",
            "Network Architecture Flow:\n",
            "  Input: (batch, 7) â†’ Feature extraction\n",
            "  Hidden: (batch, 256) â†’ LSTM processing\n",
            "  Output: (batch, 5) â†’ Actions\n",
            "âœ“ T4-Optimized Configuration loaded\n",
            "\n",
            "Key T4 Settings:\n",
            "  Batch Size: 128\n",
            "  Mixed Precision: True\n",
            "  Hidden Dim: 256\n",
            "  Buffer Size: 50,000\n",
            "  Pin Memory: True\n",
            "Trainer initialized on cuda\n",
            "Number of agents: 2\n",
            "BFT enabled: False\n",
            "STL monitoring enabled: False\n",
            "\n",
            "âœ“ MARL Trainer initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification Cell - Run this before training\n",
        "print(\"=\" * 80)\n",
        "print(\"DIMENSION VERIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a dummy batch\n",
        "dummy_batch = torch.randn(64, 7).to(device)  # batch_size=64, state_dim=7\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    action_logits, values, hidden = trainer.agents[0](dummy_batch)\n",
        "\n",
        "print(f\"âœ“ Input shape: {dummy_batch.shape}\")\n",
        "print(f\"âœ“ Action logits shape: {action_logits.shape}\")  # Should be (64, 5)\n",
        "print(f\"âœ“ Values shape: {values.shape}\")  # Should be (64, 1)\n",
        "print(f\"âœ“ Hidden h shape: {hidden[0].shape}\")  # Should be (2, 64, 256)\n",
        "print(f\"âœ“ Hidden c shape: {hidden[1].shape}\")  # Should be (2, 64, 256)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CONFIGURATION VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Dataset features: 7\")\n",
        "print(f\"Config state_dim: {config['state_dim']}\")\n",
        "print(f\"Agent state_dim: {trainer.agents[0].state_dim}\")\n",
        "print(f\"Agent hidden_dim: {trainer.agents[0].hidden_dim}\")\n",
        "print(f\"Agent action_dim: {trainer.agents[0].action_dim}\")\n",
        "\n",
        "# Check architecture\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"NETWORK ARCHITECTURE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Feature_net[0]: {trainer.agents[0].feature_net[0]}\")  # Should be Linear(7, 256)\n",
        "print(f\"LSTM: {trainer.agents[0].lstm}\")  # Should be LSTM(256, 256)\n",
        "print(f\"Actor[0]: {trainer.agents[0].actor[0]}\")  # Should be Linear(256, 256)\n",
        "print(f\"Critic[0]: {trainer.agents[0].critic[0]}\")  # Should be Linear(256, 256)\n",
        "\n",
        "print(\"\\nâœ… All dimensions validated! Ready to train.\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkkbnEoxpSxy",
        "outputId": "72ce261a-c1e7-4ac9-e809-5dcd8f63bf36"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "DIMENSION VERIFICATION\n",
            "================================================================================\n",
            "âœ“ Input shape: torch.Size([64, 7])\n",
            "âœ“ Action logits shape: torch.Size([64, 5])\n",
            "âœ“ Values shape: torch.Size([64, 1])\n",
            "âœ“ Hidden h shape: torch.Size([2, 64, 256])\n",
            "âœ“ Hidden c shape: torch.Size([2, 64, 256])\n",
            "\n",
            "================================================================================\n",
            "CONFIGURATION VALIDATION\n",
            "================================================================================\n",
            "Dataset features: 7\n",
            "Config state_dim: 7\n",
            "Agent state_dim: 7\n",
            "Agent hidden_dim: 256\n",
            "Agent action_dim: 5\n",
            "\n",
            "================================================================================\n",
            "NETWORK ARCHITECTURE\n",
            "================================================================================\n",
            "Feature_net[0]: Linear(in_features=7, out_features=256, bias=True)\n",
            "LSTM: LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
            "Actor[0]: Linear(in_features=256, out_features=256, bias=True)\n",
            "Critic[0]: Linear(in_features=256, out_features=256, bias=True)\n",
            "\n",
            "âœ… All dimensions validated! Ready to train.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "start_training",
        "outputId": "b5e9a6bb-11b9-4a94-b10f-2dac5b7f797e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STARTING MARL TRAINING\n",
            "================================================================================\n",
            "\n",
            "[Iteration 1/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.9s\n",
            "\n",
            "[Iteration 2/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.5s\n",
            "\n",
            "[Iteration 3/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 4/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 5/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "================================================================================\n",
            "EVALUATION at iteration 5\n",
            "================================================================================\n",
            "  Accuracy:  0.0000\n",
            "  Precision: 0.0000\n",
            "  Recall:    0.0000\n",
            "  F1-Score:  0.0000\n",
            "  STL Violations: 0\n",
            "================================================================================\n",
            "\n",
            "\n",
            "[Iteration 6/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 7/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 8/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 9/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 10/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "Iteration 10/20\n",
            "  Reward: 241.50\n",
            "  Accuracy: 0.6390\n",
            "  F1-Score: 0.1395\n",
            "  STL Violations: 0\n",
            "  Policy Loss: 0.0332\n",
            "  Value Loss: 903.5521\n",
            "  Constraint Loss: 0.2189\n",
            "\n",
            "================================================================================\n",
            "EVALUATION at iteration 10\n",
            "================================================================================\n",
            "  Accuracy:  0.0000\n",
            "  Precision: 0.0000\n",
            "  Recall:    0.0000\n",
            "  F1-Score:  0.0000\n",
            "  STL Violations: 0\n",
            "================================================================================\n",
            "\n",
            "\n",
            "[Iteration 11/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 12/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 13/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 14/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 15/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "================================================================================\n",
            "EVALUATION at iteration 15\n",
            "================================================================================\n",
            "  Accuracy:  0.0000\n",
            "  Precision: 0.0000\n",
            "  Recall:    0.0000\n",
            "  F1-Score:  0.0000\n",
            "  STL Violations: 0\n",
            "================================================================================\n",
            "\n",
            "\n",
            "[Iteration 16/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 17/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.6s\n",
            "\n",
            "[Iteration 18/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 19/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "[Iteration 20/20] Starting...\n",
            "  Collecting trajectories...\n",
            "  âœ“ Collected in 0.4s\n",
            "\n",
            "Iteration 20/20\n",
            "  Reward: 57.00\n",
            "  Accuracy: 0.5681\n",
            "  F1-Score: 0.2203\n",
            "  STL Violations: 0\n",
            "  Policy Loss: -0.0032\n",
            "  Value Loss: 585.9781\n",
            "  Constraint Loss: 0.2340\n",
            "\n",
            "================================================================================\n",
            "EVALUATION at iteration 20\n",
            "================================================================================\n",
            "  Accuracy:  0.0000\n",
            "  Precision: 0.0000\n",
            "  Recall:    0.0000\n",
            "  F1-Score:  0.0000\n",
            "  STL Violations: 0\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED!\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "training_stats = trainer.train(\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_iterations=20,\n",
        "    eval_interval=5,\n",
        "    save_interval=5\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_section"
      },
      "source": [
        "## 8. Final Evaluation and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_eval"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL EVALUATION ON TEST SET\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "test_metrics = trainer.evaluate(test_loader, num_batches=len(test_loader))\n",
        "\n",
        "print(\"Security Performance Metrics:\")\n",
        "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
        "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
        "print(f\"  F1-Score:  {test_metrics['f1']:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"  True Positives:  {test_metrics['tp']}\")\n",
        "print(f\"  True Negatives:  {test_metrics['tn']}\")\n",
        "print(f\"  False Positives: {test_metrics['fp']}\")\n",
        "print(f\"  False Negatives: {test_metrics['fn']}\")\n",
        "\n",
        "# BFT Statistics\n",
        "if trainer.env.use_bft:\n",
        "    bft_stats = trainer.env.bft_consensus.get_statistics()\n",
        "    print(f\"\\nBFT Consensus Statistics:\")\n",
        "    print(f\"  Total Rounds:     {bft_stats['total_rounds']}\")\n",
        "    print(f\"  Success Rate:     {bft_stats['success_rate']:.2%}\")\n",
        "    print(f\"  Avg Time:         {bft_stats['avg_consensus_time']*1000:.2f} ms\")\n",
        "    print(f\"  Byzantine Tolerance: {bft_stats['max_byzantine_tolerance']} agents\")\n",
        "\n",
        "# STL Statistics\n",
        "if trainer.env.use_stl:\n",
        "    stl_stats = trainer.env.stl_monitor.get_statistics()\n",
        "    print(f\"\\nSTL Monitoring Statistics:\")\n",
        "    print(f\"  Total Steps:      {stl_stats['total_steps']}\")\n",
        "    print(f\"  Global Violations: {stl_stats['global_violations']}\")\n",
        "    print(f\"  Violation Rate:   {stl_stats['violation_rate']:.2%}\")\n",
        "\n",
        "    for spec_name in ['response_time', 'threat_detection', 'security_posture', 'resource_utilization']:\n",
        "        if f'{spec_name}_violations' in stl_stats:\n",
        "            print(f\"  {spec_name.replace('_', ' ').title()} Violations: {stl_stats[f'{spec_name}_violations']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viz_section"
      },
      "source": [
        "## 9. Visualization of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu_final_stats"
      },
      "outputs": [],
      "source": [
        "# Final GPU Statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL GPU STATISTICS (T4)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "gpu_stats = gpu_monitor.get_stats()\n",
        "print(f\"\\nAverage GPU Utilization: {gpu_stats['avg_gpu_util']:.1f}%\")\n",
        "print(f\"Peak Memory Allocated: {gpu_stats['max_memory_allocated']:.2f} GB\")\n",
        "print(f\"Average Memory Usage: {gpu_stats['avg_memory_allocated']:.2f} GB\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nCurrent GPU State:\")\n",
        "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    print(f\"  Reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "    print(f\"  Peak:      {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "# Plot GPU utilization over time\n",
        "if gpu_monitor.gpu_utilization:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    ax1.plot(gpu_monitor.gpu_utilization, linewidth=2, color='#1f77b4')\n",
        "    ax1.axhline(y=80, color='r', linestyle='--', alpha=0.5, label='80% Target')\n",
        "    ax1.set_title('T4 GPU Utilization Over Training', fontweight='bold')\n",
        "    ax1.set_xlabel('Sample')\n",
        "    ax1.set_ylabel('GPU Utilization (%)')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(gpu_monitor.memory_allocated, linewidth=2, color='#ff7f0e')\n",
        "    ax2.axhline(y=14, color='r', linestyle='--', alpha=0.5, label='14 GB (~88% of 16GB)')\n",
        "    ax2.set_title('T4 Memory Usage Over Training', fontweight='bold')\n",
        "    ax2.set_xlabel('Sample')\n",
        "    ax2.set_ylabel('Memory Allocated (GB)')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{BASE_DIR}/gpu_utilization.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nâœ“ GPU utilization plot saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('MARL Training Progress', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Episode rewards\n",
        "axes[0, 0].plot(training_stats['episode_rewards'], alpha=0.6)\n",
        "axes[0, 0].plot(\n",
        "    pd.Series(training_stats['episode_rewards']).rolling(50).mean(),\n",
        "    linewidth=2, label='Moving Average (50)'\n",
        ")\n",
        "axes[0, 0].set_title('Episode Rewards')\n",
        "axes[0, 0].set_xlabel('Iteration')\n",
        "axes[0, 0].set_ylabel('Total Reward')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Detection accuracy\n",
        "axes[0, 1].plot(training_stats['detection_accuracy'], label='Accuracy', alpha=0.8)\n",
        "axes[0, 1].plot(training_stats['precision'], label='Precision', alpha=0.8)\n",
        "axes[0, 1].plot(training_stats['recall'], label='Recall', alpha=0.8)\n",
        "axes[0, 1].plot(training_stats['f1_score'], label='F1-Score', linewidth=2)\n",
        "axes[0, 1].set_title('Detection Metrics')\n",
        "axes[0, 1].set_xlabel('Iteration')\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# STL violations\n",
        "axes[0, 2].plot(training_stats['stl_violations'], color='red', alpha=0.6)\n",
        "axes[0, 2].plot(\n",
        "    pd.Series(training_stats['stl_violations']).rolling(50).mean(),\n",
        "    linewidth=2, color='darkred', label='Moving Average (50)'\n",
        ")\n",
        "axes[0, 2].set_title('STL Constraint Violations')\n",
        "axes[0, 2].set_xlabel('Iteration')\n",
        "axes[0, 2].set_ylabel('Violations per Episode')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Policy loss\n",
        "axes[1, 0].plot(training_stats['policy_losses'], alpha=0.6)\n",
        "axes[1, 0].plot(\n",
        "    pd.Series(training_stats['policy_losses']).rolling(50).mean(),\n",
        "    linewidth=2, label='Moving Average (50)'\n",
        ")\n",
        "axes[1, 0].set_title('Policy Loss')\n",
        "axes[1, 0].set_xlabel('Iteration')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Value loss\n",
        "axes[1, 1].plot(training_stats['value_losses'], alpha=0.6, color='orange')\n",
        "axes[1, 1].plot(\n",
        "    pd.Series(training_stats['value_losses']).rolling(50).mean(),\n",
        "    linewidth=2, color='darkorange', label='Moving Average (50)'\n",
        ")\n",
        "axes[1, 1].set_title('Value Loss')\n",
        "axes[1, 1].set_xlabel('Iteration')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Constraint loss\n",
        "axes[1, 2].plot(training_stats['constraint_losses'], alpha=0.6, color='purple')\n",
        "axes[1, 2].plot(\n",
        "    pd.Series(training_stats['constraint_losses']).rolling(50).mean(),\n",
        "    linewidth=2, color='darkviolet', label='Moving Average (50)'\n",
        ")\n",
        "axes[1, 2].set_title('Constraint Loss')\n",
        "axes[1, 2].set_xlabel('Iteration')\n",
        "axes[1, 2].set_ylabel('Loss')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{BASE_DIR}/training_progress.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Ã¢Å“â€œ Training curves saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results"
      },
      "outputs": [],
      "source": [
        "# Save final results\n",
        "results = {\n",
        "    'test_metrics': test_metrics,\n",
        "    'config': config,\n",
        "    'training_stats': {\n",
        "        key: [float(v) for v in value]\n",
        "        for key, value in training_stats.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "if trainer.env.use_bft:\n",
        "    results['bft_stats'] = trainer.env.bft_consensus.get_statistics()\n",
        "\n",
        "if trainer.env.use_stl:\n",
        "    results['stl_stats'] = trainer.env.stl_monitor.get_statistics()\n",
        "\n",
        "# Save results\n",
        "with open(f'{BASE_DIR}/final_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Save models\n",
        "trainer.save_models(f'{BASE_DIR}/models/final_marl_agents')\n",
        "\n",
        "print(\"\\nÃ¢Å“â€œ Results and models saved!\")\n",
        "print(f\"  Location: {BASE_DIR}\")\n",
        "print(f\"  - final_results.json\")\n",
        "print(f\"  - training_progress.png\")\n",
        "print(f\"  - models/final_marl_agents/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_section"
      },
      "source": [
        "## 10. Summary and Next Steps\n",
        "\n",
        "### What We've Accomplished:\n",
        "\n",
        "1. **STL Framework**: Implemented Signal Temporal Logic specifications for:\n",
        "   - Response time constraints\n",
        "   - Threat detection timing\n",
        "   - Security posture maintenance\n",
        "   - Resource utilization bounds\n",
        "\n",
        "2. **BFT Consensus**: Built Byzantine Fault-Tolerant protocol with:\n",
        "   - Digital signature authentication\n",
        "   - Quorum-based voting\n",
        "   - Three-phase consensus\n",
        "   - Tolerance for up to f < n/3 Byzantine agents\n",
        "\n",
        "3. **Constraint-Aware MARL**: Developed multi-agent system with:\n",
        "   - LSTM-based agents for temporal modeling\n",
        "   - Actor-Critic architecture with constraint prediction\n",
        "   - Independent PPO training\n",
        "   - Integration of STL and BFT\n",
        "\n",
        "4. **Training Pipeline**: Complete training system with:\n",
        "   - DARPA TC dataset integration\n",
        "   - GAE for advantage estimation\n",
        "   - Comprehensive evaluation metrics\n",
        "   - Real-time monitoring and logging\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Baseline Comparison**: Compare against:\n",
        "   - Danino et al. (2023) - runtime improvement\n",
        "   - Lindemann et al. (2023) - verification latency\n",
        "   - Tunde-Onadele et al. (2024) - attack detection rates\n",
        "\n",
        "2. **Real Deployment**: Integrate with:\n",
        "   - Docker/Kubernetes\n",
        "   - CI/CD pipelines\n",
        "   - Cloud infrastructure (AWS/Azure)\n",
        "\n",
        "\n",
        "### Key Metrics to Track:\n",
        "- Detection accuracy, precision, recall, F1-score\n",
        "- STL constraint satisfaction rate\n",
        "- BFT consensus success rate\n",
        "- Response time and latency\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}