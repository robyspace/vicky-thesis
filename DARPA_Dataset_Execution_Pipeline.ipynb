{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4",
   "mount_file_id": "1Dg_pnYPic9xit0x7FxlQIG6jsEXKqEN0",
   "authorship_tag": "ABX9TyMt4au8g3ULL6Q6NOodzULu"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIiVrTrSqTLJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760528386619,
     "user_tz": -330,
     "elapsed": 6473,
     "user": {
      "displayName": "Vistronics India",
      "userId": "16074982155385528679"
     }
    },
    "outputId": "bed9b128-8cc8-4c4e-da25-ecb2466939d5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Installing required packages...\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing required packages...\")\n",
    "!pip install gdown pandas pyarrow torch torchvision torchaudio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "import gzip\n",
    "\n",
    "# For Avro binary format\n",
    "try:\n",
    "    import fastavro\n",
    "    print(\"‚úì fastavro installed successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Installing fastavro...\")\n",
    "    !pip install fastavro\n",
    "    import fastavro\n",
    "\n",
    "# Set your base directory\n",
    "BASE_DIR = \"/content/drive/MyDrive/mythesis/vicky/darpa_tc\"\n",
    "print(f\"‚úì Working directory: {BASE_DIR}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HNq1H5sqv8A",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760530232631,
     "user_tz": -330,
     "elapsed": 4519,
     "user": {
      "displayName": "Vistronics India",
      "userId": "16074982155385528679"
     }
    },
    "outputId": "bb04877c-4ca5-4288-d1e5-bcbfd1c0e597"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚ö†Ô∏è  Installing fastavro...\n",
      "Collecting fastavro\n",
      "  Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.8 kB)\n",
      "Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fastavro\n",
      "Successfully installed fastavro-1.12.1\n",
      "‚úì Working directory: /content/drive/MyDrive/mythesis/vicky/darpa_tc\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create all necessary subdirectories\"\"\"\n",
    "    dirs = {\n",
    "        'raw': f'{BASE_DIR}/raw',\n",
    "        'processed': f'{BASE_DIR}/processed',\n",
    "        'features': f'{BASE_DIR}/features',\n",
    "        'splits': f'{BASE_DIR}/splits',\n",
    "        'metadata': f'{BASE_DIR}/metadata'\n",
    "    }\n",
    "\n",
    "    for name, path in dirs.items():\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"‚úì Created: {name} ‚Üí {path}\")\n",
    "\n",
    "    return dirs\n",
    "\n",
    "dirs = setup_directories()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdjJjLbgq2MQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760528413957,
     "user_tz": -330,
     "elapsed": 2416,
     "user": {
      "displayName": "Vistronics India",
      "userId": "16074982155385528679"
     }
    },
    "outputId": "d74b432b-0402-4515-b9b2-86551f905959"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úì Created: raw ‚Üí /content/drive/MyDrive/mythesis/vicky/darpa_tc/raw\n",
      "‚úì Created: processed ‚Üí /content/drive/MyDrive/mythesis/vicky/darpa_tc/processed\n",
      "‚úì Created: features ‚Üí /content/drive/MyDrive/mythesis/vicky/darpa_tc/features\n",
      "‚úì Created: splits ‚Üí /content/drive/MyDrive/mythesis/vicky/darpa_tc/splits\n",
      "‚úì Created: metadata ‚Üí /content/drive/MyDrive/mythesis/vicky/darpa_tc/metadata\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def download_darpa_sample():\n",
    "    \"\"\"\n",
    "    Download a manageable subset of DARPA TC Engagement 5\n",
    "\n",
    "    Options:\n",
    "    1. Manual download from Google Drive link and upload to Colab\n",
    "    2. Download specific files using gdown\n",
    "    3. Use shared Drive folder directly\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DARPA TC Dataset Download Options\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nüì• OPTION 1: Manual Upload (Recommended for testing)\")\n",
    "    print(\"   1. Go to: https://drive.google.com/drive/folders/1okt4AYElyBohW4XiOBqmsvjwXsnUjLVf\")\n",
    "    print(\"   2. Download ONE file (e.g., ta1-trace-e5-official-1.json.gz)\")\n",
    "    print(\"   3. Upload to Colab using the file browser\")\n",
    "    print(f\"   4. Place in: {dirs['raw']}/\")\n",
    "\n",
    "    print(\"\\nüì• OPTION 2: Access via Shared Drive\")\n",
    "    print(\"   1. Right-click the folder in Google Drive\")\n",
    "    print(\"   2. Select 'Add shortcut to Drive'\")\n",
    "    print(\"   3. Access directly via mounted Drive\")\n",
    "\n",
    "    print(\"\\nüì• OPTION 3: Direct Download (if you have file ID)\")\n",
    "    print(\"   Use: !gdown <file_id> -O {}/sample.json.gz\".format(dirs['raw']))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"For initial testing, download just ONE host file (~5-10GB)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "download_darpa_sample()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M-wxinxJq760",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760528443700,
     "user_tz": -330,
     "elapsed": 57,
     "user": {
      "displayName": "Vistronics India",
      "userId": "16074982155385528679"
     }
    },
    "outputId": "27da5a67-d7c6-4087-be36-1aff40d73fe8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "DARPA TC Dataset Download Options\n",
      "============================================================\n",
      "\n",
      "üì• OPTION 1: Manual Upload (Recommended for testing)\n",
      "   1. Go to: https://drive.google.com/drive/folders/1okt4AYElyBohW4XiOBqmsvjwXsnUjLVf\n",
      "   2. Download ONE file (e.g., ta1-trace-e5-official-1.json.gz)\n",
      "   3. Upload to Colab using the file browser\n",
      "   4. Place in: /content/drive/MyDrive/mythesis/vicky/darpa_tc/raw/\n",
      "\n",
      "üì• OPTION 2: Access via Shared Drive\n",
      "   1. Right-click the folder in Google Drive\n",
      "   2. Select 'Add shortcut to Drive'\n",
      "   3. Access directly via mounted Drive\n",
      "\n",
      "üì• OPTION 3: Direct Download (if you have file ID)\n",
      "   Use: !gdown <file_id> -O /content/drive/MyDrive/mythesis/vicky/darpa_tc/raw/sample.json.gz\n",
      "\n",
      "============================================================\n",
      "For initial testing, download just ONE host file (~5-10GB)\n",
      "============================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def list_raw_files():\n",
    "    \"\"\"Check what files are in your raw directory\"\"\"\n",
    "    raw_dir = Path(dirs['raw'])\n",
    "    files = list(raw_dir.glob('*'))\n",
    "\n",
    "    if not files:\n",
    "        print(\"  No files found in raw directory\")\n",
    "        print(f\"   Please download data to: {raw_dir}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n‚úì Found {len(files)} file(s) in raw directory:\")\n",
    "    for i, f in enumerate(files, 1):\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   {i}. {f.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "    return files\n",
    "\n",
    "raw_files = list_raw_files()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZjvkVw7lsi2d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760529934562,
     "user_tz": -330,
     "elapsed": 45,
     "user": {
      "displayName": "Vistronics India",
      "userId": "16074982155385528679"
     }
    },
    "outputId": "8c43fce5-3b31-417d-e3bd-618b6c157487"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "‚úì Found 6 file(s) in raw directory:\n",
      "   1. .ipynb_checkpoints (0.00 MB)\n",
      "   2. ta1-trace-1-e5-official-1.bin.2.gz (270.65 MB)\n",
      "   3. ta1-trace-1-e5-official-1.bin.1.gz (266.57 MB)\n",
      "   4. ta1-trace-1-e5-official-1.bin.4.gz (272.90 MB)\n",
      "   5. ta1-trace-1-e5-official-1.bin.3.gz (273.70 MB)\n",
      "   6. ta1-trace-1-e5-official-1.bin.5.gz (259.57 MB)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def read_avro_binary(input_file, max_records=None):\n",
    "    \"\"\"\n",
    "    Read DARPA TC Avro binary files (.bin or .bin.gz)\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to .bin or .bin.gz file\n",
    "        max_records: Limit number of records (for testing)\n",
    "\n",
    "    Yields:\n",
    "        Parsed Avro records as dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    is_gzipped = str(input_file).endswith('.gz')\n",
    "\n",
    "    print(f\"Reading Avro file: {Path(input_file).name}\")\n",
    "    print(f\"Compressed: {is_gzipped}\")\n",
    "\n",
    "    try:\n",
    "        if is_gzipped:\n",
    "            with gzip.open(input_file, 'rb') as gz_file:\n",
    "                reader = fastavro.reader(gz_file)\n",
    "\n",
    "                for i, record in enumerate(reader):\n",
    "                    yield record\n",
    "\n",
    "                    if max_records and i >= max_records - 1:\n",
    "                        break\n",
    "        else:\n",
    "            with open(input_file, 'rb') as f:\n",
    "                reader = fastavro.reader(f)\n",
    "\n",
    "                for i, record in enumerate(reader):\n",
    "                    yield record\n",
    "\n",
    "                    if max_records and i >= max_records - 1:\n",
    "                        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading Avro file: {e}\")\n",
    "        raise\n"
   ],
   "metadata": {
    "id": "EZYrNt-TyDy7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def process_darpa_avro_streaming(input_file, chunk_size=50000, max_records=None, start_chunk_id=0):\n",
    "    \"\"\"\n",
    "    Process DARPA TC Avro binary logs with streaming\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to DARPA .bin or .bin.gz file\n",
    "        chunk_size: Number of records per chunk\n",
    "        max_records: Limit for testing (None = process all)\n",
    "        start_chunk_id: The starting ID for chunk numbering\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {Path(input_file).name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    chunk = []\n",
    "    chunk_counter = start_chunk_id # Initialize chunk counter with start_chunk_id\n",
    "    total_records = 0\n",
    "    error_count = 0\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    try:\n",
    "        for i, record in enumerate(read_avro_binary(input_file, max_records)):\n",
    "            try:\n",
    "                # Extract features from Avro record\n",
    "                processed = extract_apt_features_from_avro(record)\n",
    "\n",
    "                if processed:  # Only add valid records\n",
    "                    chunk.append(processed)\n",
    "\n",
    "                # Save chunk when size reached\n",
    "                if len(chunk) >= chunk_size:\n",
    "                    save_processed_chunk(chunk, chunk_counter)\n",
    "                    chunk_counter += 1\n",
    "                    total_records += len(chunk)\n",
    "                    chunk = []\n",
    "\n",
    "                    # Memory management\n",
    "                    gc.collect()\n",
    "\n",
    "                # Progress update\n",
    "                if (i + 1) % 50000 == 0:\n",
    "                    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "                    rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "                    print(f\"   Processed: {i+1:,} records | Rate: {rate:.0f} records/sec | \"\n",
    "                          f\"Chunks saved: {chunk_counter}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                if error_count < 5:\n",
    "                    print(f\"   Warning: Error processing record {i}: {str(e)[:100]}\")\n",
    "                continue\n",
    "\n",
    "        # Save remaining records\n",
    "        if chunk:\n",
    "            save_processed_chunk(chunk, chunk_counter)\n",
    "            total_records += len(chunk)\n",
    "            chunk_counter += 1\n",
    "\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚úì Processing Complete!\")\n",
    "        print(f\"  Total records: {total_records:,}\")\n",
    "        # Return the number of chunks processed for this file only\n",
    "        num_chunks_this_file = chunk_counter - start_chunk_id\n",
    "        print(f\"  Chunks created in this file: {num_chunks_this_file}\")\n",
    "        print(f\"  Errors: {error_count:,}\")\n",
    "        print(f\"  Time: {elapsed/60:.2f} minutes\")\n",
    "        if elapsed > 0:\n",
    "            print(f\"  Avg rate: {total_records/elapsed:.0f} records/sec\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        return num_chunks_this_file, total_records\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File not found: {input_file}\")\n",
    "        return 0, 0\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0, 0"
   ],
   "metadata": {
    "id": "ECBVGdqDsqBO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def inspect_avro_records(input_file, num_samples=5):\n",
    "    \"\"\"\n",
    "    Inspect the structure of Avro records to understand the schema\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"INSPECTING AVRO RECORD STRUCTURE\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    for i, record in enumerate(read_avro_binary(input_file, max_records=num_samples)):\n",
    "        print(f\"Record #{i+1}:\")\n",
    "        print(f\"  Type: {type(record)}\")\n",
    "        print(f\"  Keys: {list(record.keys())}\")\n",
    "\n",
    "        # Print first level structure\n",
    "        for key, value in record.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"  {key}: dict with keys {list(value.keys())[:5]}\")\n",
    "            elif isinstance(value, list):\n",
    "                print(f\"  {key}: list with {len(value)} items\")\n",
    "            else:\n",
    "                print(f\"  {key}: {type(value).__name__} = {str(value)[:100]}\")\n",
    "\n",
    "        print(f\"\\nFull record structure:\")\n",
    "        import json\n",
    "        print(json.dumps(record, indent=2, default=str)[:1000])\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "        if i >= num_samples - 1:\n",
    "            break\n",
    "\n",
    "    print(\"=\"*60 + \"\\n\")\n"
   ],
   "metadata": {
    "id": "v4pFi91_7kXw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_apt_features_from_avro(record):\n",
    "    \"\"\"\n",
    "    Extract security-relevant features from DARPA TC Avro records\n",
    "    Schema: CDM20 with top-level 'type' and nested 'datum'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Get record type from top level\n",
    "        record_type = record.get('type', 'unknown')\n",
    "        datum = record.get('datum', {})\n",
    "\n",
    "        if not datum:\n",
    "            return None\n",
    "\n",
    "        # Initialize with safe defaults\n",
    "        features = {\n",
    "            'record_type': record_type,\n",
    "            'cdm_version': str(record.get('CDMVersion', '20')),\n",
    "            'source': record.get('source', ''),\n",
    "            'session_number': record.get('sessionNumber', 0),\n",
    "            'timestamp_ns': 0,\n",
    "            'event_id': '',\n",
    "            'sequence': 0,\n",
    "            'thread_id': 0,\n",
    "            'subject_uuid': '',\n",
    "            'object_uuid': '',\n",
    "            'event_type': '',\n",
    "            'src_addr': '',\n",
    "            'src_port': 0,\n",
    "            'dst_addr': '',\n",
    "            'dst_port': 0,\n",
    "            'ip_protocol': 0,\n",
    "            'file_path': '',\n",
    "            'file_size': 0,\n",
    "            'process_pid': 0,\n",
    "            'process_ppid': 0,\n",
    "            'memory_address': 0,\n",
    "            'protection': '',\n",
    "            'is_suspicious': False\n",
    "        }\n",
    "\n",
    "        # Extract UUID (bytes format)\n",
    "        if 'uuid' in datum:\n",
    "            uuid_bytes = datum.get('uuid')\n",
    "            if isinstance(uuid_bytes, bytes):\n",
    "                features['event_id'] = uuid_bytes.hex()\n",
    "            else:\n",
    "                features['event_id'] = str(uuid_bytes)\n",
    "\n",
    "        # Extract timestamp\n",
    "        if 'timestampNanos' in datum:\n",
    "            features['timestamp_ns'] = datum.get('timestampNanos', 0)\n",
    "\n",
    "        # Extract sequence\n",
    "        if 'sequence' in datum:\n",
    "            features['sequence'] = datum.get('sequence', 0)\n",
    "\n",
    "        # Extract thread ID\n",
    "        if 'threadId' in datum:\n",
    "            features['thread_id'] = datum.get('threadId', 0)\n",
    "\n",
    "        # Process based on record type\n",
    "        if record_type == 'RECORD_EVENT':\n",
    "            # Event records\n",
    "            features['event_type'] = datum.get('type', '')\n",
    "\n",
    "            # Subject (process UUID)\n",
    "            if 'subject' in datum:\n",
    "                subject = datum.get('subject')\n",
    "                if isinstance(subject, bytes):\n",
    "                    features['subject_uuid'] = subject.hex()\n",
    "                else:\n",
    "                    features['subject_uuid'] = str(subject)\n",
    "\n",
    "            # Predicate object (what the event operates on)\n",
    "            if 'predicateObject' in datum:\n",
    "                pred_obj = datum.get('predicateObject')\n",
    "                if pred_obj:\n",
    "                    if isinstance(pred_obj, bytes):\n",
    "                        features['object_uuid'] = pred_obj.hex()\n",
    "                    else:\n",
    "                        features['object_uuid'] = str(pred_obj)\n",
    "\n",
    "            # Size (for file operations)\n",
    "            if 'size' in datum and datum.get('size'):\n",
    "                features['file_size'] = datum.get('size', 0)\n",
    "\n",
    "            # Properties (additional metadata)\n",
    "            props = datum.get('properties', {})\n",
    "            if props:\n",
    "                # Protection for memory operations\n",
    "                if 'protection' in props:\n",
    "                    features['protection'] = str(props.get('protection', ''))\n",
    "\n",
    "                # File path\n",
    "                if 'path' in props:\n",
    "                    features['file_path'] = str(props.get('path', ''))\n",
    "\n",
    "                # Network info\n",
    "                if 'remoteAddress' in props:\n",
    "                    features['dst_addr'] = str(props.get('remoteAddress', ''))\n",
    "                if 'remotePort' in props:\n",
    "                    features['dst_port'] = int(props.get('remotePort', 0))\n",
    "                if 'localAddress' in props:\n",
    "                    features['src_addr'] = str(props.get('localAddress', ''))\n",
    "                if 'localPort' in props:\n",
    "                    features['src_port'] = int(props.get('localPort', 0))\n",
    "\n",
    "        elif record_type == 'RECORD_SUBJECT':\n",
    "            # Subject/Process records\n",
    "            features['event_type'] = datum.get('type', '')\n",
    "\n",
    "            base_obj = datum.get('baseObject', {})\n",
    "            if base_obj:\n",
    "                props = base_obj.get('properties', {})\n",
    "                if props:\n",
    "                    if 'pid' in props:\n",
    "                        features['process_pid'] = int(props.get('pid', 0))\n",
    "                    if 'ppid' in props:\n",
    "                        features['process_ppid'] = int(props.get('ppid', 0))\n",
    "                    if 'tgid' in props:\n",
    "                        features['thread_id'] = int(props.get('tgid', 0))\n",
    "\n",
    "        elif record_type == 'RECORD_FILE_OBJECT':\n",
    "            # File object records\n",
    "            base_obj = datum.get('baseObject', {})\n",
    "            if base_obj:\n",
    "                props = base_obj.get('properties', {})\n",
    "                if props and 'path' in props:\n",
    "                    features['file_path'] = str(props.get('path', ''))\n",
    "\n",
    "            if 'size' in datum:\n",
    "                features['file_size'] = datum.get('size', 0)\n",
    "\n",
    "        elif record_type == 'RECORD_NETFLOW_OBJECT':\n",
    "            # Network flow records\n",
    "            base_obj = datum.get('baseObject', {})\n",
    "            if base_obj:\n",
    "                props = base_obj.get('properties', {})\n",
    "                if props:\n",
    "                    features['src_addr'] = str(props.get('localAddress', ''))\n",
    "                    features['src_port'] = int(props.get('localPort', 0))\n",
    "                    features['dst_addr'] = str(props.get('remoteAddress', ''))\n",
    "                    features['dst_port'] = int(props.get('remotePort', 0))\n",
    "                    features['ip_protocol'] = int(props.get('ipProtocol', 0))\n",
    "\n",
    "        elif record_type == 'RECORD_MEMORY_OBJECT':\n",
    "            # Memory object records\n",
    "            if 'memoryAddress' in datum:\n",
    "                features['memory_address'] = datum.get('memoryAddress', 0)\n",
    "            if 'size' in datum:\n",
    "                features['file_size'] = datum.get('size', 0)\n",
    "\n",
    "            base_obj = datum.get('baseObject', {})\n",
    "            if base_obj:\n",
    "                props = base_obj.get('properties', {})\n",
    "                if props and 'tgid' in props:\n",
    "                    features['process_pid'] = int(props.get('tgid', 0))\n",
    "\n",
    "        # Detect suspicious behavior\n",
    "        features['is_suspicious'] = detect_suspicious_behavior(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error but still return basic structure\n",
    "        return None\n",
    "\n",
    "def extract_event_features(event_data):\n",
    "    \"\"\"Extract features from Event records\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    features['timestamp_ns'] = event_data.get('timestampNanos', 0)\n",
    "    features['sequence'] = event_data.get('sequence', 0)\n",
    "    features['event_id'] = str(event_data.get('uuid', ''))\n",
    "\n",
    "    # Subject (process/principal)\n",
    "    subject_uuid = event_data.get('subject')\n",
    "    if subject_uuid:\n",
    "        features['subject_uuid'] = str(subject_uuid)\n",
    "\n",
    "    # Predicate Object (what the event operates on)\n",
    "    pred_obj = event_data.get('predicateObject')\n",
    "    if pred_obj:\n",
    "        features['object_uuid'] = str(pred_obj)\n",
    "\n",
    "    # Predicate (operation type)\n",
    "    predicate = event_data.get('predicateObjectPath')\n",
    "    if predicate:\n",
    "        features['predicate_type'] = str(predicate)\n",
    "\n",
    "    # Operation type\n",
    "    event_type = event_data.get('type')\n",
    "    if event_type:\n",
    "        features['operation'] = str(event_type)\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_subject_features(subject_data):\n",
    "    \"\"\"Extract features from Subject (process) records\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    features['timestamp_ns'] = subject_data.get('timestampNanos', 0)\n",
    "    features['subject_uuid'] = str(subject_data.get('uuid', ''))\n",
    "\n",
    "    # Process properties\n",
    "    properties = subject_data.get('properties', {})\n",
    "    if properties:\n",
    "        features['process_pid'] = properties.get('map', {}).get('pid', 0)\n",
    "        features['process_ppid'] = properties.get('map', {}).get('ppid', 0)\n",
    "        features['process_name'] = properties.get('map', {}).get('name', '')\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_file_features(file_data):\n",
    "    \"\"\"Extract features from FileObject records\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    features['timestamp_ns'] = file_data.get('timestampNanos', 0)\n",
    "    features['object_uuid'] = str(file_data.get('uuid', ''))\n",
    "\n",
    "    # File properties\n",
    "    properties = file_data.get('properties', {})\n",
    "    if properties:\n",
    "        prop_map = properties.get('map', {})\n",
    "        features['file_path'] = prop_map.get('path', '')\n",
    "        features['file_size'] = int(prop_map.get('size', 0)) if prop_map.get('size') else 0\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_network_features(net_data):\n",
    "    \"\"\"Extract features from NetFlowObject records\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    features['timestamp_ns'] = net_data.get('timestampNanos', 0)\n",
    "    features['object_uuid'] = str(net_data.get('uuid', ''))\n",
    "\n",
    "    # Network properties\n",
    "    properties = net_data.get('properties', {})\n",
    "    if properties:\n",
    "        prop_map = properties.get('map', {})\n",
    "        features['src_addr'] = prop_map.get('srcAddress', '')\n",
    "        features['src_port'] = int(prop_map.get('srcPort', 0)) if prop_map.get('srcPort') else 0\n",
    "        features['dst_addr'] = prop_map.get('destAddress', '')\n",
    "        features['dst_port'] = int(prop_map.get('destPort', 0)) if prop_map.get('destPort') else 0\n",
    "        features['ip_protocol'] = int(prop_map.get('ipProtocol', 0)) if prop_map.get('ipProtocol') else 0\n",
    "\n",
    "    return features\n",
    "\n",
    "def detect_suspicious_behavior(features):\n",
    "    \"\"\"\n",
    "    Enhanced heuristic for flagging potentially suspicious activity\n",
    "    Based on DARPA TC APT detection scenarios\n",
    "    \"\"\"\n",
    "    suspicious = False\n",
    "\n",
    "    # Suspicious network activity\n",
    "    dst_port = features.get('dst_port', 0)\n",
    "    if dst_port in [4444, 31337, 1337, 8080, 9999, 6666, 1234]:\n",
    "        suspicious = True\n",
    "\n",
    "    # Suspicious file operations\n",
    "    file_path = features.get('file_path', '').lower()\n",
    "    suspicious_paths = ['/tmp/', '/dev/shm/', 'powershell', 'wget', 'curl',\n",
    "                        '.sh', 'base64', '/etc/passwd', '/etc/shadow']\n",
    "    if any(sp in file_path for sp in suspicious_paths):\n",
    "        suspicious = True\n",
    "\n",
    "    # Suspicious event types\n",
    "    event_type = features.get('event_type', '')\n",
    "    suspicious_events = ['EVENT_EXECUTE', 'EVENT_MMAP', 'EVENT_CLONE',\n",
    "                         'EVENT_LOADLIBRARY', 'EVENT_CREATE_THREAD']\n",
    "    if event_type in suspicious_events:\n",
    "        suspicious = True\n",
    "\n",
    "    # Memory protection changes (common in exploits)\n",
    "    protection = features.get('protection', '')\n",
    "    if protection in ['7', '5']:  # RWX or R-X permissions\n",
    "        suspicious = True\n",
    "\n",
    "    # Unusual process relationships\n",
    "    if features.get('process_ppid', 0) == 1 and features.get('process_pid', 0) > 1000:\n",
    "        # Process reparented to init (orphaned)\n",
    "        suspicious = True\n",
    "\n",
    "    return suspicious"
   ],
   "metadata": {
    "id": "D9r9XqjCs2Ll"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def save_processed_chunk(chunk, chunk_id):\n",
    "    \"\"\"Save processed chunk efficiently using Parquet format\"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(chunk)\n",
    "\n",
    "        # Convert timestamp to datetime\n",
    "        if 'timestamp_ns' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp_ns'], unit='ns', errors='coerce')\n",
    "\n",
    "        # Fill NaN values\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "        string_columns = df.select_dtypes(include=['object']).columns\n",
    "        df[string_columns] = df[string_columns].fillna('')\n",
    "\n",
    "        # Save as Parquet\n",
    "        output_file = f\"{dirs['processed']}/chunk_{chunk_id:04d}.parquet\"\n",
    "        df.to_parquet(output_file, compression='snappy', index=False)\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   Error saving chunk {chunk_id}: {e}\")\n",
    "        return False"
   ],
   "metadata": {
    "id": "kVlsv2hqs3ca"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def create_temporal_splits(processed_dir, train_ratio=0.7, val_ratio=0.15):\n    \"\"\"Create stratified splits for APT detection (ensuring consistent threat distribution)\"\"\"\n    from sklearn.model_selection import train_test_split\n\n    print(f\"\\n{'='*60}\")\n    print(\"Creating Train/Val/Test Splits (STRATIFIED)\")\n    print(f\"{'='*60}\\n\")\n\n    processed_path = Path(processed_dir)\n    parquet_files = sorted(processed_path.glob('*.parquet'))\n\n    if not parquet_files:\n        print(\"‚ùå No processed files found!\")\n        print(f\"   Expected location: {processed_dir}\")\n        return None, None, None\n\n    print(f\"‚úì Found {len(parquet_files)} chunk files\")\n\n    # Load and concatenate all chunks\n    print(\"Loading chunks...\")\n    dfs = []\n    for i, f in enumerate(parquet_files):\n        df = pd.read_parquet(f)\n        dfs.append(df)\n        if (i + 1) % 10 == 0:\n            print(f\"   Loaded {i+1}/{len(parquet_files)} chunks...\")\n\n    print(\"Concatenating data...\")\n    full_df = pd.concat(dfs, ignore_index=True)\n\n    print(f\"‚úì Total records: {len(full_df):,}\")\n\n    # CRITICAL FIX: Use stratified splitting to ensure consistent threat distribution\n    print(\"\\nüîÑ Creating stratified splits...\")\n    print(\"   This ensures all splits have the same threat distribution!\")\n\n    # Extract labels for stratification\n    labels = full_df['is_suspicious'].values\n\n    # First split: 70% train, 30% temp (val + test)\n    train_df, temp_df = train_test_split(\n        full_df,\n        test_size=(1 - train_ratio),\n        stratify=labels,  # ‚Üê CRITICAL: Ensures same threat ratio\n        random_state=42\n    )\n\n    # Second split: Split temp into val (15%) and test (15%)\n    temp_labels = temp_df['is_suspicious'].values\n    val_df, test_df = train_test_split(\n        temp_df,\n        test_size=0.5,  # 50% of temp = 15% of total\n        stratify=temp_labels,  # ‚Üê CRITICAL: Ensures same threat ratio\n        random_state=42\n    )\n\n    print(f\"‚úì Stratified splitting complete!\")\n\n    # Save splits\n    splits_dir = dirs['splits']\n    print(f\"\\nüíæ Saving splits to {splits_dir}...\")\n\n    train_df.to_parquet(f'{splits_dir}/train.parquet', index=False)\n    val_df.to_parquet(f'{splits_dir}/val.parquet', index=False)\n    test_df.to_parquet(f'{splits_dir}/test.parquet', index=False)\n\n    # Calculate metrics\n    train_threats = train_df['is_suspicious'].sum()\n    val_threats = val_df['is_suspicious'].sum()\n    test_threats = test_df['is_suspicious'].sum()\n\n    train_threat_pct = train_threats / len(train_df) * 100\n    val_threat_pct = val_threats / len(val_df) * 100\n    test_threat_pct = test_threats / len(test_df) * 100\n\n    # Save metadata\n    metadata = {\n        'total_records': len(full_df),\n        'train_records': len(train_df),\n        'val_records': len(val_df),\n        'test_records': len(test_df),\n        'train_threats': int(train_threats),\n        'val_threats': int(val_threats),\n        'test_threats': int(test_threats),\n        'train_threat_pct': float(train_threat_pct),\n        'val_threat_pct': float(val_threat_pct),\n        'test_threat_pct': float(test_threat_pct),\n        'split_method': 'stratified',\n        'random_state': 42\n    }\n\n    with open(f\"{dirs['metadata']}/split_info.json\", 'w') as f:\n        json.dump(metadata, f, indent=2)\n\n    # Print summary\n    print(f\"\\n{'='*60}\")\n    print(\"‚úì Splits Created Successfully!\")\n    print(f\"{'='*60}\")\n    print(f\"\\nüìä Dataset Statistics:\")\n    print(f\"   Total Records:     {len(full_df):,}\")\n    print(f\"   Train:             {len(train_df):,} ({train_ratio*100:.1f}%)\")\n    print(f\"   Validation:        {len(val_df):,} ({val_ratio*100:.1f}%)\")\n    print(f\"   Test:              {len(test_df):,} ({(1-train_ratio-val_ratio)*100:.1f}%)\")\n    \n    print(f\"\\nüö® Suspicious Activity Distribution (STRATIFIED):\")\n    print(f\"   Train:     {train_threats:,}/{len(train_df):,} = {train_threat_pct:.2f}%\")\n    print(f\"   Val:       {val_threats:,}/{len(val_df):,} = {val_threat_pct:.2f}%\")\n    print(f\"   Test:      {test_threats:,}/{len(test_df):,} = {test_threat_pct:.2f}%\")\n    \n    # Verify stratification worked\n    print(f\"\\n‚úÖ Verification:\")\n    if abs(train_threat_pct - val_threat_pct) < 0.5 and abs(train_threat_pct - test_threat_pct) < 0.5:\n        print(f\"   ‚úì All splits have consistent threat distribution!\")\n        print(f\"   ‚úì Difference < 0.5% between splits\")\n    else:\n        print(f\"   ‚ö†Ô∏è  Warning: Splits have different distributions!\")\n        print(f\"   ‚ö†Ô∏è  Train-Val diff: {abs(train_threat_pct - val_threat_pct):.2f}%\")\n        print(f\"   ‚ö†Ô∏è  Train-Test diff: {abs(train_threat_pct - test_threat_pct):.2f}%\")\n    \n    print(f\"{'='*60}\\n\")\n\n    return train_df, val_df, test_df",
   "metadata": {
    "id": "bDkjNJLKs8_x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class DARPAAPTDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for DARPA TC data optimized for MARL\"\"\"\n",
    "\n",
    "    def __init__(self, parquet_file, feature_columns=None):\n",
    "        print(f\"Loading dataset from {parquet_file}...\")\n",
    "        self.df = pd.read_parquet(parquet_file)\n",
    "\n",
    "        # Define features for MARL state representation\n",
    "        if feature_columns is None:\n",
    "            self.feature_columns = [\n",
    "                'sequence', 'src_port', 'dst_port',\n",
    "                'ip_protocol', 'file_size', 'process_pid', 'process_ppid'\n",
    "            ]\n",
    "        else:\n",
    "            self.feature_columns = feature_columns\n",
    "\n",
    "        # Ensure columns exist\n",
    "        for col in self.feature_columns:\n",
    "            if col not in self.df.columns:\n",
    "                self.df[col] = 0\n",
    "\n",
    "        # Fill missing values\n",
    "        for col in self.feature_columns:\n",
    "            self.df[col] = self.df[col].fillna(0)\n",
    "\n",
    "        # Normalize features\n",
    "        self.normalize_features()\n",
    "\n",
    "        print(f\"‚úì Loaded {len(self.df):,} records\")\n",
    "        print(f\"‚úì Using {len(self.feature_columns)} features\")\n",
    "\n",
    "    def normalize_features(self):\n",
    "        \"\"\"Normalize numeric features to [0, 1] range\"\"\"\n",
    "        for col in self.feature_columns:\n",
    "            max_val = self.df[col].max()\n",
    "            if max_val > 0:\n",
    "                self.df[col] = self.df[col] / max_val\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        features = []\n",
    "        for col in self.feature_columns:\n",
    "            features.append(float(row[col]))\n",
    "\n",
    "        state = torch.tensor(features, dtype=torch.float32)\n",
    "        label = torch.tensor(int(row['is_suspicious']), dtype=torch.long)\n",
    "\n",
    "        metadata = {\n",
    "            'record_type': row.get('record_type', ''),\n",
    "            'timestamp': str(row.get('timestamp', ''))\n",
    "        }\n",
    "\n",
    "        return state, label, metadata\n",
    "\n",
    "def create_data_loader(split_file, batch_size=256, shuffle=True):\n",
    "    \"\"\"Create DataLoader for MARL training\"\"\"\n",
    "    dataset = DARPAAPTDataset(split_file)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
    "\n",
    "    print(f\"‚úì DataLoader created: {len(loader)} batches\")\n",
    "    return loader, dataset\n"
   ],
   "metadata": {
    "id": "9Gu8wVNDtFQJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2861f2a5"
   },
   "source": [
    "## Modify `run complete pipeline`\n",
    "\n",
    "### Subtask:\n",
    "Adjust the `run_complete_pipeline` function to only perform the processing step using `process_darpa_avro_streaming` and save the processed chunks, without proceeding to create splits or dataloaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7776d84d"
   },
   "source": [
    "def run_complete_pipeline(raw_file_path, test_mode=False, start_chunk_id=0):\n",
    "    \"\"\"\n",
    "    Execute the data processing pipeline step for a single raw file.\n",
    "\n",
    "    Args:\n",
    "        raw_file_path: Path to DARPA TC .bin or .bin.gz file\n",
    "        test_mode: If True, process only 100K records for testing\n",
    "        start_chunk_id: The starting ID for chunk numbering (to avoid overwriting)\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        tuple: (num_chunks, num_records) processed from the file,\n",
    "               or (0, 0) if processing fails.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"DARPA TC DATA PROCESSING STEP\")\n",
    "\n",
    "    max_records = 100000 if test_mode else None\n",
    "\n",
    "    # Step 1: Process raw Avro logs\n",
    "    print(\"STEP 1: Processing Avro binary logs...\")\n",
    "    num_chunks, num_records = process_darpa_avro_streaming(\n",
    "        raw_file_path,\n",
    "        chunk_size=50000,\n",
    "        max_records=max_records,\n",
    "        start_chunk_id=start_chunk_id # Pass the start chunk ID\n",
    "    )\n",
    "\n",
    "    if num_records == 0:\n",
    "        print(\" Processing failed for this file.\")\n",
    "        return 0, 0\n",
    "\n",
    "    print(\"PROCESSING STEP COMPLETE FOR THIS FILE!\")\n",
    "\n",
    "    return num_chunks, num_records"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b1c9328"
   },
   "source": [
    "## Iterate and process\n",
    "\n",
    "### Subtask:\n",
    "Loop through each raw file in the raw directory, calling the `run_complete_pipeline` function to process each file individually. Accumulate the results (number of chunks and records) from each file.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07af91af",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760535654528,
     "user_tz": -330,
     "elapsed": 14576,
     "user": {
      "displayName": "Vistronics India",
      "userId": "16074982155385528679"
     }
    },
    "outputId": "30b35faa-6f33-47a5-e70f-98e9f5ba420a"
   },
   "source": [
    "# Get the list of raw files\n",
    "raw_files = list_raw_files()\n",
    "\n",
    "# Initialize variables\n",
    "total_chunks_processed = 0\n",
    "total_records_processed = 0\n",
    "\n",
    "# Initialize a global chunk counter\n",
    "global_chunk_counter = 0\n",
    "\n",
    "# Iterate through the list of raw files\n",
    "for raw_file in raw_files:\n",
    "    # Skip directory files like .ipynb_checkpoints\n",
    "    if raw_file.is_dir():\n",
    "        print(f\"Skipping directory: {raw_file.name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING FILE: {raw_file.name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Run the complete pipeline for the current file in test mode\n",
    "    # Pass the current global chunk counter to the processing function\n",
    "    num_chunks, num_records = run_complete_pipeline(str(raw_file), test_mode=True, start_chunk_id=global_chunk_counter)\n",
    "\n",
    "    # Update the global chunk counter\n",
    "    global_chunk_counter += num_chunks\n",
    "\n",
    "    # Add the returned num_chunks and num_records to the total\n",
    "    total_chunks_processed += num_chunks\n",
    "    total_records_processed += num_records\n",
    "\n",
    "    # Free up memory\n",
    "    gc.collect()\n",
    "\n",
    "# After the loop, print the total number of chunks and records processed\n",
    "print(f\"\\nFinished processing all raw files.\")\n",
    "print(f\"Total chunks processed across all files: {total_chunks_processed}\")\n",
    "print(f\"Total records processed across all files: {total_records_processed:,}\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "‚úì Found 6 file(s) in raw directory:\n",
      "   1. .ipynb_checkpoints (0.00 MB)\n",
      "   2. ta1-trace-1-e5-official-1.bin.2.gz (270.65 MB)\n",
      "   3. ta1-trace-1-e5-official-1.bin.1.gz (266.57 MB)\n",
      "   4. ta1-trace-1-e5-official-1.bin.4.gz (272.90 MB)\n",
      "   5. ta1-trace-1-e5-official-1.bin.3.gz (273.70 MB)\n",
      "   6. ta1-trace-1-e5-official-1.bin.5.gz (259.57 MB)\n",
      "Skipping directory: .ipynb_checkpoints\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FILE: ta1-trace-1-e5-official-1.bin.2.gz\n",
      "================================================================================\n",
      "\n",
      "DARPA TC DATA PROCESSING STEP\n",
      "STEP 1: Processing Avro binary logs...\n",
      "\n",
      "============================================================\n",
      "Processing: ta1-trace-1-e5-official-1.bin.2.gz\n",
      "============================================================\n",
      "\n",
      "Reading Avro file: ta1-trace-1-e5-official-1.bin.2.gz\n",
      "Compressed: True\n",
      "   Processed: 50,000 records | Rate: 34182 records/sec | Chunks saved: 1\n",
      "   Processed: 100,000 records | Rate: 34640 records/sec | Chunks saved: 2\n",
      "\n",
      "============================================================\n",
      "‚úì Processing Complete!\n",
      "  Total records: 100,000\n",
      "  Chunks created in this file: 2\n",
      "  Errors: 0\n",
      "  Time: 0.05 minutes\n",
      "  Avg rate: 34632 records/sec\n",
      "============================================================\n",
      "\n",
      "PROCESSING STEP COMPLETE FOR THIS FILE!\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FILE: ta1-trace-1-e5-official-1.bin.1.gz\n",
      "================================================================================\n",
      "\n",
      "DARPA TC DATA PROCESSING STEP\n",
      "STEP 1: Processing Avro binary logs...\n",
      "\n",
      "============================================================\n",
      "Processing: ta1-trace-1-e5-official-1.bin.1.gz\n",
      "============================================================\n",
      "\n",
      "Reading Avro file: ta1-trace-1-e5-official-1.bin.1.gz\n",
      "Compressed: True\n",
      "   Processed: 50,000 records | Rate: 35425 records/sec | Chunks saved: 3\n",
      "   Processed: 100,000 records | Rate: 35114 records/sec | Chunks saved: 4\n",
      "\n",
      "============================================================\n",
      "‚úì Processing Complete!\n",
      "  Total records: 100,000\n",
      "  Chunks created in this file: 2\n",
      "  Errors: 0\n",
      "  Time: 0.05 minutes\n",
      "  Avg rate: 35107 records/sec\n",
      "============================================================\n",
      "\n",
      "PROCESSING STEP COMPLETE FOR THIS FILE!\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FILE: ta1-trace-1-e5-official-1.bin.4.gz\n",
      "================================================================================\n",
      "\n",
      "DARPA TC DATA PROCESSING STEP\n",
      "STEP 1: Processing Avro binary logs...\n",
      "\n",
      "============================================================\n",
      "Processing: ta1-trace-1-e5-official-1.bin.4.gz\n",
      "============================================================\n",
      "\n",
      "Reading Avro file: ta1-trace-1-e5-official-1.bin.4.gz\n",
      "Compressed: True\n",
      "   Processed: 50,000 records | Rate: 35068 records/sec | Chunks saved: 5\n",
      "   Processed: 100,000 records | Rate: 35058 records/sec | Chunks saved: 6\n",
      "\n",
      "============================================================\n",
      "‚úì Processing Complete!\n",
      "  Total records: 100,000\n",
      "  Chunks created in this file: 2\n",
      "  Errors: 0\n",
      "  Time: 0.05 minutes\n",
      "  Avg rate: 35051 records/sec\n",
      "============================================================\n",
      "\n",
      "PROCESSING STEP COMPLETE FOR THIS FILE!\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FILE: ta1-trace-1-e5-official-1.bin.3.gz\n",
      "================================================================================\n",
      "\n",
      "DARPA TC DATA PROCESSING STEP\n",
      "STEP 1: Processing Avro binary logs...\n",
      "\n",
      "============================================================\n",
      "Processing: ta1-trace-1-e5-official-1.bin.3.gz\n",
      "============================================================\n",
      "\n",
      "Reading Avro file: ta1-trace-1-e5-official-1.bin.3.gz\n",
      "Compressed: True\n",
      "   Processed: 50,000 records | Rate: 36092 records/sec | Chunks saved: 7\n",
      "   Processed: 100,000 records | Rate: 35932 records/sec | Chunks saved: 8\n",
      "\n",
      "============================================================\n",
      "‚úì Processing Complete!\n",
      "  Total records: 100,000\n",
      "  Chunks created in this file: 2\n",
      "  Errors: 0\n",
      "  Time: 0.05 minutes\n",
      "  Avg rate: 35926 records/sec\n",
      "============================================================\n",
      "\n",
      "PROCESSING STEP COMPLETE FOR THIS FILE!\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FILE: ta1-trace-1-e5-official-1.bin.5.gz\n",
      "================================================================================\n",
      "\n",
      "DARPA TC DATA PROCESSING STEP\n",
      "STEP 1: Processing Avro binary logs...\n",
      "\n",
      "============================================================\n",
      "Processing: ta1-trace-1-e5-official-1.bin.5.gz\n",
      "============================================================\n",
      "\n",
      "Reading Avro file: ta1-trace-1-e5-official-1.bin.5.gz\n",
      "Compressed: True\n",
      "   Processed: 50,000 records | Rate: 36109 records/sec | Chunks saved: 9\n",
      "   Processed: 100,000 records | Rate: 35786 records/sec | Chunks saved: 10\n",
      "\n",
      "============================================================\n",
      "‚úì Processing Complete!\n",
      "  Total records: 100,000\n",
      "  Chunks created in this file: 2\n",
      "  Errors: 0\n",
      "  Time: 0.05 minutes\n",
      "  Avg rate: 35780 records/sec\n",
      "============================================================\n",
      "\n",
      "PROCESSING STEP COMPLETE FOR THIS FILE!\n",
      "\n",
      "Finished processing all raw files.\n",
      "Total chunks processed across all files: 10\n",
      "Total records processed across all files: 500,000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6eaac70"
   },
   "source": [
    "## Create combined splits\n",
    "\n",
    "### Subtask:\n",
    "Create a single set of train, validation, and test splits from the combined processed data stored in the `processed` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "860b1f66",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760534261927,
     "user_tz": -330,
     "elapsed": 1477,
     "user": {
      "displayName": "Vistronics India",
      "userId": "16074982155385528679"
     }
    },
    "outputId": "de797191-800e-4b10-b0cd-59ed8ef9c77c"
   },
   "source": [
    "# Step 3: Create splits from the combined processed data\n",
    "print(\"\\nSTEP 3: Creating train/val/test splits from combined data...\")\n",
    "combined_train_df, combined_val_df, combined_test_df = create_temporal_splits(dirs['processed'])\n",
    "\n",
    "if combined_train_df is None:\n",
    "    print(\" Pipeline failed: Could not create combined splits\")\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "STEP 3: Creating train/val/test splits from combined data...\n",
      "\n",
      "============================================================\n",
      "Creating Train/Val/Test Splits\n",
      "============================================================\n",
      "\n",
      "Found 10 chunk files\n",
      "Loading chunks...\n",
      "   Loaded 10/10 chunks...\n",
      "Concatenating data...\n",
      "Total records: 500,000\n",
      "Sorting by timestamp...\n",
      "Splitting data...\n",
      "Saving splits to /content/drive/MyDrive/mythesis/vicky/darpa_tc/splits...\n",
      "\n",
      "============================================================\n",
      "‚úì Splits Created Successfully!\n",
      "============================================================\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "   Total Records:     500,000\n",
      "   Train:             350,000 (70.0%)\n",
      "   Validation:        75,000 (15.0%)\n",
      "   Test:              75,000 (15.0%)\n",
      "\n",
      "üïê Time Ranges:\n",
      "   Train:     1970-01-01 00:00:00 ‚Üí 2019-05-07 20:01:38.246000\n",
      "   Val:       2019-05-07 20:01:38.246000 ‚Üí 2019-05-07 20:01:45.530000\n",
      "   Test:      2019-05-07 20:01:45.530000 ‚Üí 2019-05-07 21:54:19.668000\n",
      "\n",
      "üö® Suspicious Activity Rates:\n",
      "   Train:     9.97%\n",
      "   Val:       6.09%\n",
      "   Test:      20.22%\n",
      "============================================================\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d418a513"
   },
   "source": [
    "## Create combined dataloaders\n",
    "\n",
    "### Subtask:\n",
    "Create the PyTorch DataLoaders from the combined split files.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0f4d6ab",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760534269586,
     "user_tz": -330,
     "elapsed": 374,
     "user": {
      "displayName": "Vistronics India",
      "userId": "16074982155385528679"
     }
    },
    "outputId": "bb97ca51-76fd-4688-f7fe-bbf4e6d94070"
   },
   "source": [
    "# Step 4: Create dataloaders from combined splits\n",
    "print(\"\\nSTEP 4: Creating PyTorch DataLoaders from combined splits...\")\n",
    "\n",
    "combined_train_loader, _ = create_data_loader(f\"{dirs['splits']}/train.parquet\", batch_size=256, shuffle=True)\n",
    "combined_val_loader, _ = create_data_loader(f\"{dirs['splits']}/val.parquet\", batch_size=256, shuffle=False)\n",
    "combined_test_loader, _ = create_data_loader(f\"{dirs['splits']}/test.parquet\", batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"DATALOADER CREATION COMPLETE!\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "STEP 4: Creating PyTorch DataLoaders from combined splits...\n",
      "Loading dataset from /content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/train.parquet...\n",
      "‚úì Loaded 350,000 records\n",
      "‚úì Using 7 features\n",
      "‚úì DataLoader created: 1368 batches\n",
      "Loading dataset from /content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/val.parquet...\n",
      "‚úì Loaded 75,000 records\n",
      "‚úì Using 7 features\n",
      "‚úì DataLoader created: 293 batches\n",
      "Loading dataset from /content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/test.parquet...\n",
      "‚úì Loaded 75,000 records\n",
      "‚úì Using 7 features\n",
      "‚úì DataLoader created: 293 batches\n",
      "DATALOADER CREATION COMPLETE!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bea0efa"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "*   Five raw data files were processed sequentially.\n",
    "*   In test mode, each file processed 100,000 records, resulting in 2 chunks per file.\n",
    "*   A total of 10 chunks and 500,000 records were processed across all files.\n",
    "*   The combined processed data was successfully split into train (350,000 records), validation (75,000 records), and test (75,000 records) sets using a temporal split strategy.\n",
    "*   PyTorch DataLoaders were successfully created for the combined train, validation, and test splits with a batch size of 256.\n",
    "\n",
    "\n",
    "*   The created DataLoaders (`combined_train_loader`, `combined_val_loader`, `combined_test_loader`) are now ready for use in training and evaluating our proposed model.\n"
   ]
  }
 ]
}