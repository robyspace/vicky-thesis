{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1Qh4XHsUi4yDv_Mxof3BuElALTmUwXQUZ","authorship_tag":"ABX9TyOlN6QY/whtZdaz2GGEdG/W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLrY5jiOjL6O","executionInfo":{"status":"ok","timestamp":1760527040958,"user_tz":-330,"elapsed":209,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}},"outputId":"964e76ed-be31-43b9-dc29-7a82c1fa4020"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Oct 15 11:17:20 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n","Filesystem      Size  Used Avail Use% Mounted on\n","overlay         236G   40G  197G  17% /\n","tmpfs            64M     0   64M   0% /dev\n","shm              25G     0   25G   0% /dev/shm\n","/dev/root       2.0G  1.2G  750M  62% /usr/sbin/docker-init\n","tmpfs            26G   48K   26G   1% /var/colab\n","/dev/sda1       242G   42G  201G  18% /kaggle/input\n","tmpfs            26G     0   26G   0% /proc/acpi\n","tmpfs            26G     0   26G   0% /proc/scsi\n","tmpfs            26G     0   26G   0% /sys/firmware\n","drive           236G   50G  187G  21% /content/drive\n"]}],"source":["!nvidia-smi  # Check GPU availability\n","!df -h      # Check disk space (Colab has ~100GB)"]},{"cell_type":"code","source":["import os\n","import gdown\n","import pandas as pd\n","import json\n","from pathlib import Path\n","import subprocess"],"metadata":{"id":"ndouytFhl2sn","executionInfo":{"status":"ok","timestamp":1760527195825,"user_tz":-330,"elapsed":629,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def setup_environment():\n","    \"\"\"Mount Google Drive and create working directories\"\"\"\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Create directory structure\n","    directories = [\n","        '/content/drive/MyDrive/mythesis/vicky/darpa_tc/raw',\n","        '/content/drive/MyDrive/mythesis/vicky/darpa_tc/processed',\n","        '/content/drive/MyDrive/mythesis/vicky/darpa_tc/features',\n","        '/content/drive/MyDrive/mythesis/vicky/darpa_tc/splits'\n","    ]\n","\n","    for directory in directories:\n","        Path(directory).mkdir(parents=True, exist_ok=True)\n","\n","    print(\"✓ Environment setup complete\")\n","    return directories"],"metadata":{"id":"QjjfVmMPmSeD","executionInfo":{"status":"ok","timestamp":1760527934166,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def download_darpa_engagement5(drive_folder_id, output_dir='/content/drive/MyDrive/mythesis/vicky/darpa_tc/raw'):\n","    \"\"\"\n","    Download DARPA TC Engagement 5 dataset\n","\n","    Args:\n","        drive_folder_id: ID from Google Drive URL\n","        output_dir: Local directory to save files\n","    \"\"\"\n","\n","    # For large datasets, download specific files rather than entire folder\n","    # You can list files first to see what's available\n","\n","    print(\"Downloading DARPA TC Engagement 5...\")\n","\n","    # Option 1: Download entire folder (if permissions allow)\n","    try:\n","        gdown.download_folder(\n","            id=drive_folder_id,\n","            output=output_dir,\n","            quiet=False,\n","            use_cookies=False\n","        )\n","        print(f\"✓ Dataset downloaded to {output_dir}\")\n","    except Exception as e:\n","        print(f\"Folder download failed: {e}\")\n","        print(\"Try downloading individual files or use rclone method below\")"],"metadata":{"id":"LtuBJznrm6PH","executionInfo":{"status":"ok","timestamp":1760527935759,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def setup_rclone_transfer(gdrive_folder_url):\n","    \"\"\"\n","    Setup rclone for efficient large file transfers\n","    More reliable than gdown for huge datasets\n","    \"\"\"\n","\n","    # Install rclone\n","    !curl https://rclone.org/install.sh | sudo bash\n","\n","    # Configure rclone for Google Drive\n","    print(\"Configure rclone with your Google account:\")\n","    print(\"1. Run: rclone config\")\n","    print(\"2. Select 'n' for new remote\")\n","    print(\"3. Name it 'gdrive'\")\n","    print(\"4. Select Google Drive\")\n","    print(\"5. Follow OAuth flow\")\n","    print(\"\\nThen use: rclone copy gdrive:path/to/dataset /content/drive/MyDrive/mythesis/vicky/darpa_tc/raw -P\")\n"],"metadata":{"id":"XRMolZYmnHf1","executionInfo":{"status":"ok","timestamp":1760527948244,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def process_darpa_logs_streaming(input_file, chunk_size=100000):\n","    \"\"\"\n","    Process DARPA logs in streaming fashion to avoid memory issues\n","    DARPA TC uses JSON format for provenance graphs\n","    \"\"\"\n","\n","    processed_records = []\n","\n","    print(f\"Processing {input_file} in chunks of {chunk_size}...\")\n","\n","    with open(input_file, 'r') as f:\n","        chunk = []\n","        for i, line in enumerate(f):\n","            try:\n","                record = json.loads(line.strip())\n","\n","                # Extract relevant features for APT detection\n","                processed = extract_features(record)\n","                chunk.append(processed)\n","\n","                # Process in batches to manage memory\n","                if len(chunk) >= chunk_size:\n","                    df = pd.DataFrame(chunk)\n","                    save_processed_chunk(df, i // chunk_size)\n","                    chunk = []\n","\n","                if i % 1000000 == 0:\n","                    print(f\"Processed {i:,} records...\")\n","\n","            except json.JSONDecodeError:\n","                continue\n","\n","    # Process remaining records\n","    if chunk:\n","        df = pd.DataFrame(chunk)\n","        save_processed_chunk(df, 'final')\n","\n","    print(\"✓ Streaming processing complete\")\n"],"metadata":{"id":"tpjLXwGEnW0A","executionInfo":{"status":"ok","timestamp":1760527951275,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def extract_features(record):\n","    \"\"\"\n","    Extract security-relevant features from DARPA TC provenance records\n","    Aligned with your MARL + STL security framework\n","    \"\"\"\n","\n","    features = {\n","        # Temporal features\n","        'timestamp': record.get('timestamp_nanos', 0) / 1e9,\n","\n","        # Process features\n","        'process_id': record.get('subject', {}).get('uuid', ''),\n","        'process_name': record.get('subject', {}).get('properties', {}).get('name', ''),\n","\n","        # Network features (for container security)\n","        'src_ip': record.get('predicateObject', {}).get('properties', {}).get('src_address', ''),\n","        'dst_ip': record.get('predicateObject', {}).get('properties', {}).get('dst_address', ''),\n","        'dst_port': record.get('predicateObject', {}).get('properties', {}).get('dst_port', 0),\n","\n","        # File operations\n","        'file_path': record.get('predicateObject2', {}).get('properties', {}).get('path', ''),\n","        'operation': record.get('type', ''),\n","\n","        # Security-relevant flags\n","        'is_privileged': record.get('subject', {}).get('properties', {}).get('privileged', False),\n","        'sequence_id': record.get('sequence', 0)\n","    }\n","\n","    return features"],"metadata":{"id":"Ft8p_kpwnY7d","executionInfo":{"status":"ok","timestamp":1760527953875,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def create_temporal_splits(processed_dir, train_ratio=0.7, val_ratio=0.15):\n","    \"\"\"\n","    Create temporally-aware splits (important for APT detection)\n","    Maintains chronological order to prevent data leakage\n","    \"\"\"\n","\n","    # Load all processed chunks\n","    all_files = sorted(Path(processed_dir).glob('*.parquet'))\n","    dfs = [pd.read_parquet(f) for f in all_files]\n","    df = pd.concat(dfs, ignore_index=True)\n","\n","    # Sort by timestamp\n","    df = df.sort_values('timestamp')\n","\n","    # Split temporally\n","    n = len(df)\n","    train_end = int(n * train_ratio)\n","    val_end = int(n * (train_ratio + val_ratio))\n","\n","    train_df = df.iloc[:train_end]\n","    val_df = df.iloc[train_end:val_end]\n","    test_df = df.iloc[val_end:]\n","\n","    # Save splits\n","    train_df.to_parquet('/content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/train.parquet')\n","    val_df.to_parquet('/content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/val.parquet')\n","    test_df.to_parquet('/content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/test.parquet')\n","\n","    print(f\"✓ Splits created:\")\n","    print(f\"  Train: {len(train_df):,} records\")\n","    print(f\"  Val:   {len(val_df):,} records\")\n","    print(f\"  Test:  {len(test_df):,} records\")\n","\n","    return train_df, val_df, test_df"],"metadata":{"id":"TGc_hVfUngGu","executionInfo":{"status":"ok","timestamp":1760527956095,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def create_data_loader(split_file, batch_size=256):\n","    \"\"\"\n","    Create memory-efficient data loader for MARL training\n","    Compatible with PyTorch/TensorRL\n","    \"\"\"\n","    import torch\n","    from torch.utils.data import Dataset, DataLoader\n","\n","    class DARPADataset(Dataset):\n","        def __init__(self, parquet_file):\n","            self.df = pd.read_parquet(parquet_file)\n","\n","        def __len__(self):\n","            return len(self.df)\n","\n","        def __getitem__(self, idx):\n","            row = self.df.iloc[idx]\n","\n","            # Convert to tensors for MARL\n","            features = torch.tensor([\n","                row['dst_port'],\n","                row['is_privileged'],\n","                row['sequence_id']\n","                # Add more numeric features\n","            ], dtype=torch.float32)\n","\n","            return features\n","\n","    dataset = DARPADataset(split_file)\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    return loader"],"metadata":{"id":"Txm-j8Eonrxi","executionInfo":{"status":"ok","timestamp":1760527958700,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def main():\n","    \"\"\"Complete data preparation pipeline\"\"\"\n","\n","    print(\"=== DARPA TC Dataset Preparation Pipeline ===\\n\")\n","\n","    # Step 1: Setup\n","    setup_environment()\n","\n","    # Step 2: Download dataset\n","    # Extract folder ID from your Google Drive URL\n","    # https://drive.google.com/drive/folders/1okt4AYElyBohW4XiOBqmsvjwXsnUjLVf\n","    folder_id = \"1okt4AYElyBohW4XiOBqmsvjwXsnUjLVf\"\n","\n","    print(\"\\n⚠️  DARPA Engagement 5 is very large (100GB+)\")\n","    print(\"Recommended approach:\")\n","    print(\"1. Download a subset first (e.g., one day's data)\")\n","    print(\"2. Test your pipeline\")\n","    print(\"3. Scale to full dataset once validated\\n\")\n","\n","    # Uncomment when ready to download\n","    # download_darpa_engagement5(folder_id)\n","\n","    print(\"Pipeline setup complete!\")\n","    print(\"Next steps:\")\n","    print(\"1. Download subset of data\")\n","    print(\"2. Run: process_darpa_logs_streaming('path/to/log.json')\")\n","    print(\"3. Run: create_temporal_splits('/content/drive/MyDrive/mythesis/vicky/darpa_tc/processed')\")\n","    print(\"4. Run: create_data_loader('/content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/train.parquet')\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTMkbsaWnvv_","executionInfo":{"status":"ok","timestamp":1760527969651,"user_tz":-330,"elapsed":2007,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}},"outputId":"463daf25-4c31-48c3-e434-9ed811023d46"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["=== DARPA TC Dataset Preparation Pipeline ===\n","\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✓ Environment setup complete\n","\n","⚠️  DARPA Engagement 5 is very large (100GB+)\n","Recommended approach:\n","1. Download a subset first (e.g., one day's data)\n","2. Test your pipeline\n","3. Scale to full dataset once validated\n","\n","Pipeline setup complete!\n","Next steps:\n","1. Download subset of data\n","2. Run: process_darpa_logs_streaming('path/to/log.json')\n","3. Run: create_temporal_splits('/content/drive/MyDrive/mythesis/vicky/darpa_tc/processed')\n","4. Run: create_data_loader('/content/drive/MyDrive/mythesis/vicky/darpa_tc/splits/train.parquet')\n"]}]}]}